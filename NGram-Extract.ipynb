{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import codecs\n",
    "import sys\n",
    "import pickle\n",
    "#import time\n",
    "\n",
    "debug=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "def warning(*objs):\n",
    "    print(\"WARNING: \", *objs, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nomFeatures=\"NGrams-LetterFeatures.csv\"\n",
    "with codecs.open(nomFeatures,\"r\",encoding=\"utf8\") as stream:\n",
    "    featureLines=stream.readlines()\n",
    "letterFeatures={}\n",
    "for featureLine in featureLines[2:]:\n",
    "    elements=featureLine.strip().split(\"\\t\")\n",
    "    letterFeatures[elements[0]]=[int(i) for i in elements[1:]]\n",
    "#letterFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorizeNGram(ngram):\n",
    "    vecteurs=[]\n",
    "    for letter in ngram:\n",
    "        vecteurs.extend(letterFeatures[letter])\n",
    "    return vecteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vectorizeNGram(\"bac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nomFichier=\"LesMiserables.txt\"\n",
    "try:\n",
    "    fichier = codecs.open(nomFichier, \"r\",encoding=\"utf8\")\n",
    "except IOError:\n",
    "    warning(\"could not open\", nomFichier)\n",
    "    sys.exit()\n",
    "else:\n",
    "    text = fichier.readlines()\n",
    "    fichier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ngramN = 5\n",
    "pc = [\".\", \",\", \";\", \":\", \"?\", \"!\", \"--\", \"...\", u\"«\", u\"»\", \"(\", \")\", \"_\",\"*\",'\"',\"/\",\"+\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normaliserTexte(text,nospace=False):\n",
    "    normalText=[]\n",
    "    for line in text:\n",
    "        for ponct in pc: \n",
    "            line=line.replace(ponct,\"\")\n",
    "        line=line.replace(\"'\",\" \")\n",
    "        line=line.replace(\"-\",\" \")\n",
    "        if nospace:\n",
    "            line=re.sub('\\s+','',line)\n",
    "        else:\n",
    "            line=re.sub('\\s+',' ',line)\n",
    "        normalText.append(line.lower())\n",
    "    return normalText\n",
    "\n",
    "def extraireNgrams(text,noclasses=False):\n",
    "    ngrams=[]\n",
    "    classes=[]\n",
    "    for line in text[:]:\n",
    "        line = line.strip()\n",
    "    #    print (line)\n",
    "        espace = line.count(\" \")\n",
    "#        print (len(line),ngramN,espace)\n",
    "        i = 0\n",
    "        while i <= (len(line) - ngramN - espace):\n",
    "            if line[i] != \" \":\n",
    "                lettres = 0\n",
    "                lettreslocales = 0\n",
    "                ngramLocal=[]\n",
    "                j = 0\n",
    "                type = \"\"\n",
    "                while lettres < ngramN:\n",
    "                    if line[i + j] != \" \":\n",
    "                        ngramLocal.append(line[i + j])\n",
    "                        lettres += 1\n",
    "                        lettreslocales += 1\n",
    "                    else:\n",
    "                        type = type + str(lettreslocales) + \"_\"\n",
    "                        lettreslocales = 0\n",
    "                    j += 1\n",
    "                type = type + str(lettreslocales)\n",
    "                ngrams.append(ngramLocal)\n",
    "                classes.append(type)\n",
    "            i = i + 1\n",
    "    if noclasses:\n",
    "        return ngrams\n",
    "    else:\n",
    "        return(ngrams, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u't', u'o', u'm', u'e', u'i'] 4_1\n"
     ]
    }
   ],
   "source": [
    "trainTexte = normaliserTexte(text)\n",
    "(ngramTrain,classesTrain)=extraireNgrams(trainTexte)\n",
    "print (ngramTrain[0],classesTrain[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngramTrain2=[]\n",
    "for n,ngram in enumerate(ngramTrain):\n",
    "    if debug: print (n, \"\".join(ngram),end=\", \")\n",
    "    ngramTrain2.append(vectorizeNGram(ngram))\n",
    "    \n",
    "#ngramTrain2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "#clf = svm.SVC(gamma=0.001, C=100.)\n",
    "clf = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "clf.fit(ngramTrain2, classesTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#testSource=[u\"Dans cette histoire, il y a un loup, une chêvre, et un agneau. Tout se passe bien tant que le loup et l'agneau ne sont pas seuls.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nomFichier=\"texte.txt\"\n",
    "try:\n",
    "    fichier = codecs.open(nomFichier, \"r\", encoding=\"utf-8\")\n",
    "except IOError:\n",
    "    warning(\"could not open\", nomFichier)\n",
    "    sys.exit()\n",
    "else:\n",
    "    testSource = fichier.readlines()\n",
    "    fichier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testTexte=normaliserTexte(testSource,True)\n",
    "ngramTest=extraireNgrams(testTexte,True)\n",
    "print (ngramTest[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ngramTest2=[]\n",
    "for n,ngram in enumerate(ngramTest):\n",
    "    if n>2500000: print (n, \"\".join(ngram),end=\", \")\n",
    "    ngramTest2.append(vectorizeNGram(ngram))\n",
    "    \n",
    "#ngramTrain2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction=clf.predict(ngramTest2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultChunks=[]\n",
    "for i,element in enumerate(prediction):\n",
    "    chunks=[int(n) for n in element.split(\"_\")]\n",
    "#    print (chunks)\n",
    "    debut=0\n",
    "    elementChunks=[]\n",
    "    for chunk in chunks:\n",
    "#        print (chunk,ngramTest[i][debut:debut+chunk],end=\",\")\n",
    "#    print ()\n",
    "        elementChunks.append(ngramTest[i][debut:debut+chunk])\n",
    "#        print (ngramTest[i][debut:debut+chunk],end=\" \")\n",
    "        debut+=chunk\n",
    "#    print ()\n",
    "    resultChunks.append(elementChunks)\n",
    "#print (resultChunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testSpaces={}\n",
    "for i,chunks in enumerate(resultChunks):\n",
    "    posChunk=0\n",
    "    for chunk in chunks:\n",
    "        if posChunk!=0: \n",
    "#            print (i+posChunk,chunk)\n",
    "            if not i+posChunk in testSpaces:\n",
    "                testSpaces[i+posChunk]=0\n",
    "            testSpaces[i+posChunk]+=1\n",
    "        posChunk+=len(chunk)\n",
    "#testSpaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for i,lettre in enumerate(testTexte):\n",
    "#    print (i)\n",
    "    if i in testSpaces:\n",
    "        # choisir le seuil d'apparition des espaces\n",
    "        # au dessus de deux types convergents : >2\n",
    "        # on insère un espace\n",
    "#        print (testSpaces[i])        \n",
    "        if testSpaces[i]>2:\n",
    "            print (\"_\",end=\"\")\n",
    "        # sinon, par exemple, on colle les lettres\n",
    "        # on pourrait aussi mettre le nombre de votes convergents\n",
    "        # pour voir ce que ça donne\n",
    "        else:\n",
    "            pass\n",
    "#            print (testSpaces[i],end=\"\")\n",
    "    print (lettre,end=\"\")\n",
    "#    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testTexte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction=clf.predict(skTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngramComplete=ngramTrain+ngramTest\n",
    "textComplete=[\"\".join(x) for x in ngramComplete]\n",
    "textTrain=[\"\".join(x) for x in ngramTrain]\n",
    "textTest=[\"\".join(x) for x in ngramTest]\n",
    "print (len(textComplete),len(textTrain),len(textTest))\n",
    "print (textComplete[0],textTrain[0],textTest[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer permet de traiter le texte comme des données numériques  \n",
    "- analyzer permet de définir le niveau de tokenization (word, char)\n",
    "- ngram_range permet de définir les ngrams qui seront extraits (min, max)\n",
    "\n",
    "La méthode fit_transform permet de déterminer les données numériques (features) qui seront utilisées pour représenter le texte.\n",
    "\n",
    "Ensuite, on peut utiliser la méthode transform pour représenter un texte en utilisant ces features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "transformVectors = CountVectorizer(analyzer=u'char',ngram_range=(5, 5))\n",
    "skComplete = transformVectors.fit_transform(textComplete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM est une des formes d'apprentissage\n",
    "- gamma définit le nombre de features qui seront utilisées pour l'apprentissage\n",
    "- C ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma=0.001, C=100.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skTrain est la représentation numérique de textTrain\n",
    "\n",
    "clf.fit permet de faire l'apprentissage du lien entre les données de skTrain et les étiquettes fournies dataClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "skTrain=transformVectors.transform(textTrain)\n",
    "clf.fit(skTrain, classesTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('SVC-Decoupages.pkl', 'wb') as output:\n",
    "   pickle.dump(clf, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skTest est la représentation numérique de textTest\n",
    "\n",
    "clf.predict permet de faire la prédiction des étiquettes pour les données de skTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skTest=transformVectors.transform(textTest)\n",
    "prediction=clf.predict(skTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##interprétation des résultats\n",
    "\n",
    "###Découpage des ngrams\n",
    "\n",
    "découpage des ngrams de test en fonction des étiquettes => resultChunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultChunks=[]\n",
    "for i,element in enumerate(prediction):\n",
    "    chunks=[int(n) for n in element.split(\"_\")]\n",
    "#    print (chunks)\n",
    "    debut=0\n",
    "    elementChunks=[]\n",
    "    for chunk in chunks:\n",
    "        elementChunks.append(textTest[i][debut:debut+chunk])\n",
    "#        print (textTest[i][debut:debut+chunk],end=\" \")\n",
    "        debut+=chunk\n",
    "#    print ()\n",
    "    resultChunks.append(elementChunks)\n",
    "#print (resultChunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Calcul des votes\n",
    "\n",
    "calcul des votes pour les séparations entre mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testSpaces={}\n",
    "for i,chunks in enumerate(resultChunks):\n",
    "    posChunk=0\n",
    "    for chunk in chunks:\n",
    "        if posChunk!=0: \n",
    "#            print (i+posChunk,chunk)\n",
    "            if not i+posChunk in testSpaces:\n",
    "                testSpaces[i+posChunk]=0\n",
    "            testSpaces[i+posChunk]+=1\n",
    "        posChunk+=len(chunk)\n",
    "#testSpaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Découpage de test\n",
    "\n",
    "Séparation des mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,lettre in enumerate(testTexte[200:250]):\n",
    "    if i in testSpaces:\n",
    "        # choisir le seuil d'apparition des espaces\n",
    "        # au dessus de deux types convergents : >2\n",
    "        # on insère un espace\n",
    "        if testSpaces[i]>2:\n",
    "            print (\"_\",end=\"\")\n",
    "        # sinon, par exemple, on colle les lettres\n",
    "        # on pourrait aussi mettre le nombre de votes convergents\n",
    "        # pour voir ce que ça donne\n",
    "        else:\n",
    "            pass\n",
    "#            print (testSpaces[i],end=\"\")\n",
    "    print (lettre,end=\"\")\n",
    "#    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
