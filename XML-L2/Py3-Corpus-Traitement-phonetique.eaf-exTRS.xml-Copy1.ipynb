{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /opt/anaconda3/lib/python3.11/site-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement des EAF avec BDLexique\n",
    "- la phonétisation dans un fichier-phonetique.eaf\n",
    "- les éléments de BDLexique et les balises complémentaires dans un fichier.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time, sys, codecs, re, glob\n",
    "import pdb # ajouter pdb.set_trace() à l'endroit où on veut le débugueur\n",
    "from lxml import etree as ET\n",
    "import bs4\n",
    "# import xml.etree.cElementTree as ET\n",
    "import os, fnmatch, unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparatifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "annee=22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créer un parser XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = ET.XMLParser(remove_blank_text=True)\n",
    "debug=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lire BDLexique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fichierLexique=\"/Users/gilles/ownCloud/Cours/Bordeaux/L2-XML/XML-Ressources/bdlexique.txt\"\n",
    "fichier_exceptions=True\n",
    "\n",
    "lexicon=codecs.open(fichierLexique,\"r\",encoding='utf8')\n",
    "lBdlexique=lexicon.readlines()\n",
    "lexicon.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bdlexique={}\n",
    "for line in lBdlexique:\n",
    "    line=line.strip()\n",
    "    p=line.split(u';')\n",
    "    if p[2]==\"@\" and not p[3] in [\"N\",\"V\",\"J\",\"K\"]:\n",
    "        p[1]+=p[2]\n",
    "        p[2]=\"\"\n",
    "        if len(p)<7:\n",
    "            for i in range(len(p)+1,7):\n",
    "                p.append(\"\")\n",
    "    bdlexique[p[0].lower()]=(p[0],p[1],p[2],p[3],p[4],p[5],p[6],p[7],p[8],p[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "connecteurs=[\n",
    "    u\"et\", u\"alors\", u\"du coup\", u\"sinon\", u\"par contre\", u\"ça veut dire\", u\"enfin\",\n",
    "u\"après\", u\"donc\", u\"puisque\", u\"puisqu'\", u\"en fait\", u\"mais\", u\"parce que\", u\"parce qu'\", u\"même si\" , u\"d'abord\", u\"et puis\"\n",
    "]\n",
    "motTheme=[u\"cible\",u\"frontières\",u\"accueillir\",u\"démonstrations\",\n",
    "          u\"accompagner\",u\"contre-courant\",u\"importantes\",u\"droit de bouger\",\n",
    "          u\"peur\",u\"solution\",u\"durable\",u\"refuge\",u\"assassins\",u\"tueurs\",u\"violeurs\",\n",
    "          u\"fuient\",u\"misère\",u\"chaos\",u\"circulation\",u\"liberté\",u\"installation\",\n",
    "          u\"solidarité\",u\"migrants\",u\"ouverture\",u\"frontières\",u\"conditions\",u\"réflexe\",\n",
    "          u\"normal\",u\"sain\",u\"accueillir\",u\"moyens\",u\"intégrés\",u\"flux\",u\"migratoires\",\n",
    "          u\"énormes\",u\"moyens\",u\"structures\",u\"loger\",u\"répondre\",u\"peuples\",u\"coopération\",\n",
    "          u\"indéniable\",u\"réfugiés\",u\"digne\",u\"prendre la main\",u\"équilibrée\",u\"accueil\",\n",
    "          u\"correctement\",u\"favorable\",u\"hébergés\",u\"mise à l’abri\",u\"centre humanitaire\",\n",
    "          u\"situation\",u\"convenable\",u\"heurtée\",u\"valeurs\",u\"république\",u\"comportement\",\n",
    "          u\"politique\",u\"associations\",u\"hébergement\",u\"respect\",u\"lien\",u\"confiance\",\n",
    "          u\"travailleur\",u\"social\",u\"policiers\",u\"gazent\",u\"détruisent\",u\"nourriture\",\n",
    "          u\"refusent\",u\"points d’eau\",u\"négation\",u\"partent\",u\"parcours\",u\"migration\",\n",
    "          u\"terribles\",u\"touche\",u\"cimetière\",u\"choquant\",u\"scandaleux\",u\"dignité\",\n",
    "          u\"impression\",u\"responsabilités\",u\"multiplication\",u\"conflits\",u\"excessivement\",\n",
    "          u\"violents\",u\"solution\",u\"seule\",u\"quitter\",u\"victimes\",u\"esclavage\",\n",
    "          u\"regression\",u\"humanité\",u\"guerre\",u\"état\",u\"pauvreté\",u\"pays\",u\"conditions\",\n",
    "          u\"respect\",u\"organisme\",u\"ficher\",u\"contrôler\",u\"demande\",u\"papiers\",u\"droits\",\n",
    "          u\"échec\",u\"aide\",u\"développement\",u\"vivre\",u\"famille\",u\"terrain\",u\"voir\",\n",
    "          u\"souffrance\",u\"regression\",u\"France\",u\"richesse\",u\"humanité\",u\"extraordinaire\",\n",
    "          u\"intérger\",u\"asile\",u\"question\",u\"temporaire\",u\"débat\",u\"souhaitons\",\n",
    "          u\"propositions\",u\"européen\",u\"rapprocher\",u\"proches\",u\"position\",u\"absurdité\",\n",
    "          u\"ONG\",u\"aider\",u\"détresse\",u\"lois\",u\"règles\",u\"principes\",u\"humanité\",\n",
    "          u\"vulnérable\",u\"enfants\",u\"sauver\",u\"vies\",u\"générosité\",u\"classes\",\n",
    "          u\"francophones\",u\"découverte\",u\"langue\",u\"logements\",u\"associations\",\n",
    "          u\"continuerai\",u\"nouveaux\",u\"clé\",u\"chômage\",u\"héberger\",u\"employer\",u\"former\",\n",
    "          u\"éduquer\",u\"milliers\",u\"vaincre\",u\"réticences\",u\"acccepter\",u\"aménage\",\n",
    "          u\"refuges\",u\"régions\",u\"viennent\",u\"zones\",u\"protégées\",u\"normales\",u\"chez-eux\",\n",
    "          u\"circuler\",u\"librement\",u\"Allemagne\",u\"obligations\",u\"humanitaire\",u\"préserver\",\n",
    "          u\"populations\",u\"bataille\",u\"hégémonique\",u\"bataille\",u\"s'installer\",u\"mois\",\n",
    "          u\"désaccord\",u\"moins\",u\"accueillant\",u\"bataille\",u\"idéologique\",u\"pression\",\n",
    "          u\"chercher\",u\"immigrés\",u\"de force\",u\"délogés\",u\"expulsés\",u\"xénophobie\",\n",
    "          u\"racistes\",u\"recul\",u\"idées\",u\"argent\",u\"dramatique\",u\"problèmes\",u\"économiques\",\n",
    "          u\"interventions\",u\"militaires\",u\"famine\",u\"massacres\",u\"répression\",u\"bombardé\",\n",
    "          u\"manqué\",u\"devoir\",u\"indigne\",u\"déposer\",u\"demande d'asile\",u\"absurde\",\n",
    "          u\"campements\",u\"regrette\",u\"dispositifs\",u\"pérennes\",u\"divergence\",\n",
    "          u\"appréciation\",u\"reconstitution\",u\"campements\",u\"crise\",u\"emballement\",\n",
    "          u\"otage\",u\"noyer\",u\"guerre\",u\"bombardement\",u\"morts\",u\"difficultés\",u\"blocage\",\n",
    "          u\"maltraiter\",u\"seul\",u\"privation\",u\"absurde\",u\"illusion\",u\"mensonge\",u\"ghettos\",\n",
    "          u\"français\",u\"divisés\",u\"meurent\",u\"fuient\",u\"devoirs\",u\"moraux\",\n",
    "          u\"centres de rétention\",u\"protection\",u\"cause\",u\"guerre\",u\"lieu\",u\"couteux\",\n",
    "          u\"charge\",u\"services\",u\"gratuits\",u\"système\",u\"gestion\",u\"patrimoine\",\n",
    "          u\"nationale\",u\"importante\",u\"fossé\",u\"travail\",u\"difficulté\",u\"maîtrise\",\n",
    "          u\"suspendre\",u\"shengen\",u\"arrêter\",u\"immigration\",u\"communautarisme\",\n",
    "          u\"fondamentalisme\",u\"islamiste\",u\"gel\",u\"constructions\",u\"mosquées\",u\"islam\",\n",
    "          u\"radicaux\",u\"vivier\",u\"radicalisation\",u\"soldats\",u\"djihadisme\",u\"terroristes\",\n",
    "          u\"puissance\",u\"terreau\",u\"frappent\",u\"clandestine\",u\"massive\",u\"insécurité\",\n",
    "          u\"terrifiante\",u\"forces de l'ordre\",u\"mesures législatives\",u\"suppression\",\n",
    "          u\"enrayer\",u\"processus\",u\"récupérer\",u\"clandestins\",u\"absence\",u\"bateaux\",\n",
    "          u\"en mer\",u\"ramener\",u\"facilite\",u\"réseaux\",u\"criminels\",u\"laisser\",u\"mourir\",\n",
    "          u\"sauver\",u\"ramener\",u\"port d’origine\",u\"corrompus\",u\"passeurs\",u\"arme\",\n",
    "          u\"catastrophique\",u\"occupation\",u\"illégale\",u\"inexpulsable\",u\"engrenage\",\n",
    "          u\"procédures\",u\"demandeurs\",u\"déboutés\",u\"économique\",u\"raisons\",u\"traverser\",\n",
    "          u\"sortis\",u\"territoire\",u\"arrivés\",u\"origine\",u\"critères\",u\"extrêmement\",\n",
    "          u\"incitatif\",u\"centres\",u\"AME\",u\"CMU\",u\"ensemble\",u\"folle\",u\"coût\",u\"faramineux\",\n",
    "          u\"malvenu\",u\"opposition\",u\"opposée\",u\"communautarisme\",u\"fondamentalisme\",\n",
    "          u\"imposer\",u\"ennemi\",u\"contre\",u\"rejet\",u\"anti-démocratique\",u\"enfermemement\",\n",
    "          u\"séparation\",u\"pourissement\",u\"étranger\",u\"drame\",u\"faillite\",u\"déloyale\",\n",
    "          u\"ménage\",\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatage divers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def time2TRS(time):\n",
    "    return str(float(time)/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gestion API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "consonnes=['k\"', '(kt)\"', 'n\"', 'p\"', 'R\"', '@t\"', 't\"', '-V', '+V', '@z\"', 'z\"']\n",
    "voyelles=[\"H\", \"j\", \"w\", \"E\", \"a\", \"2\", \"9\", \"6\", \"@\", \"y\", \"u\", \"O\", u\"ò\", \"o\", \"e\", u\"è\", u\"ê\", u\"û\", u\"ô\", \"i\"]\n",
    "\n",
    "# traduire SAMPA-BDLex en API\n",
    "\n",
    "def sampa2api(sampa):\n",
    "    if isinstance(sampa,str):\n",
    "        # api=sampa.decode(\"utf8\")\n",
    "        api=sampa\n",
    "    else:\n",
    "        api=sampa\n",
    "    api=api.replace(u'n\"',u'n') \n",
    "    api=api.replace(u't\"',u't') \n",
    "    api=api.replace(u'z\"',u'z') \n",
    "    api=api.replace(u'R\"',u'ʁ') \n",
    "    api=api.replace(u'p\"',u'p') \n",
    "    api=api.replace(u'S',u'ʃ') \n",
    "    api=api.replace(u'Z',u'ʒ')\n",
    "    api=api.replace(u'N',u'ŋ')\n",
    "    api=api.replace(u'J',u'ɲ')\n",
    "    api=api.replace(u'r',u'ʁ') \n",
    "    api=api.replace(u'H',u'ɥ')\n",
    "    api=api.replace(u'E',u'ɛ')\n",
    "    api=api.replace(u'2',u'ø')\n",
    "    api=api.replace(u'9',u'œ')\n",
    "    api=api.replace(u'6',u'ə')\n",
    "    api=api.replace(u'O',u'ɔ')\n",
    "    api=api.replace(u'è',u'e')   \n",
    "    api=api.replace(u'ò',u'o')    \n",
    "    api=api.replace(u'â',u'ɑ̃')   \n",
    "    api=api.replace(u'ê',u'ɛ̃')   \n",
    "    api=api.replace(u'û',u'œ̃')  \n",
    "    api=api.replace(u'ô',u'ɔ̃')       \n",
    "    api=api.replace(u'@',u'ə')\n",
    "#     api=api.replace(u'R',u'ʁ') \n",
    "    return api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gestion Liaison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liaison possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "algorithme\n",
    "\n",
    "+ si le mot courant et le suivant ne sont pas dans lexicon, pas de liaison\n",
    "+ si le mot a une consonne dans le champ de la voyelle de liaison, check1 est vrai\n",
    "+ si le mot suivant commence par une voyelle, check2 est vrai\n",
    "\n",
    "  si check1 et check2 sont vrais, il y a liaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def formerLiaison(mot1,mot2,contexteLiaison=False):\n",
    "    if mot1[1][1]:\n",
    "        result=sampa2api(mot1[1][1])\n",
    "        if contexteLiaison:\n",
    "#             print (mot1,) \n",
    "#             print (mot2)\n",
    "            if mot1[1][2] in consonnes and mot2[1][1][0] in voyelles:\n",
    "                result+=sampa2api(mot1[1][2].strip(\"()\"))\n",
    "    else:\n",
    "        result=\"***%s***\"%mot1[1][0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tɛkstɔ̃\n"
     ]
    }
   ],
   "source": [
    "print (formerLiaison([\"textons\",(\"1\",\"tEkstô\",\"3\")],None,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liaison obligatoire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "algorithme\n",
    "\n",
    "+ si le mot courant et le suivant sont dans un des cas de figure, il y a liaison\n",
    "+ sinon pas de liaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def liaison_obligatoire(mot1,mot2):\n",
    "    determinant=[\"d\", \"P\"]\n",
    "    nom=[\"N\", \"G\", \"M\"]\n",
    "    adjectif=[\"J\", \"G\", \"M\"]\n",
    "    pronompers=[\"P\"]\n",
    "    verbe=[\"V\"]\n",
    "    cat1=mot1[1][3]\n",
    "    cat2=mot2[1][3]\n",
    "#     print (cat1,cat2)\n",
    "\n",
    "    if cat1 in determinant and cat2 in nom :\n",
    "        return True\n",
    "\n",
    "    elif cat1 in determinant and cat2 in adjectif :\n",
    "        return True\n",
    "\n",
    "    elif cat1 in adjectif and cat2 in nom :\n",
    "        return True\n",
    "    \n",
    "    elif cat1 in pronompers and cat2 in verbe :\n",
    "        return True\n",
    "\n",
    "    elif cat1 in verbe and cat2 in pronompers :\n",
    "        return True\n",
    "\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cas de figure possibles:\n",
    "\n",
    "- DET + N\n",
    "    * ri + N:   d'animal, \n",
    "    * di + N:   certains éléphants\n",
    "    * rd + N:   les animaux\n",
    "    * dd + N:   ces étés, cet été\n",
    "    * dp + N:   ton anorak\n",
    "    * rc + N:   aux armes\n",
    "- DET + ADJ:\n",
    "    * ri + ADJ:   d'énormes\n",
    "    * di + ADJ:   plusieurs immenses\n",
    "    * rd + ADJ:   les immenses\n",
    "    * dd + ADJ:   cet immense\n",
    "    * dp + ADJ:   son immense\n",
    "    * rc + ADJ:   aux immenses\n",
    "- PERS + V:\n",
    "    * SS + V:   m'épate\n",
    "- V + PRO PERS: \n",
    "    * V + SS:   vont-ils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "algorithme\n",
    "\n",
    "+ si le mot courant et le suivant sont dans un des cas de figure, il y a liaison\n",
    "+ sinon pas de liaison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liaison facultative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vérifier si la liaison est facultative\n",
    "def liaison_facultative(mot1,mot2):\n",
    "    #pdb.set_trace()\n",
    "    nom=[\"N\", \"G\", \"M\"]\n",
    "    pluriel=[\"MP\", \"FP\"]\n",
    "    adjectif=[\"J\", \"G\", \"M\"]\n",
    "    verbe=[\"V\"]\n",
    "    pronompers=[\"P\"]\n",
    "    adverbe=[\"A\"]\n",
    "    preposition=[\"p\"]\n",
    "    cat1=mot1[1][3]\n",
    "    cat2=mot2[1][3]\n",
    "    \n",
    "    if (cat1 in nom) and (mot1[1][4] in pluriel) and (cat2 in adjectif) : \n",
    "        return True\n",
    "\n",
    "    elif (cat1 in verbe) and (cat2 not in pronompers):\n",
    "        return True\n",
    "\n",
    "    elif cat1 in adverbe :\n",
    "        return True\n",
    "    \n",
    "    elif cat1 in preposition : \n",
    "        return True\n",
    "\n",
    "    else :\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cas de figure possibles :\n",
    "\n",
    "- N pl + ADJ: \n",
    "    * N + ADJ: monstres énormes \n",
    "    * G + ADJ: rivaux énormes\n",
    "- VERBE + TOUT-SAUF-PRO-PERS:\n",
    "    * V + N sont éléphants\n",
    "    * V + G sommes abdicaires\n",
    "    * V + V sommes assis\n",
    "    * V + A sommes admirablement\n",
    "    * V + p sommes autour de\n",
    "    * V + di ont aucune\n",
    "    * V + rc sommes au\n",
    "- ADV + QQCH:\n",
    "    * ADV + N vraiment abruti\n",
    "    * ADV + G vraiment abandonné\n",
    "    * ADV + V vraiment aimé\n",
    "    * ADV + J vraiment étonnant\n",
    "    * ADV + ss vraiment ils\n",
    "    * ADV + A vraiment étonnamment\n",
    "    * ADV + p vraiment attendu\n",
    "    * ADV + di vraiment autre \n",
    "    * ADV + rc vraiment au\n",
    "- PREP + QQCH:\n",
    "    * PREP + N très amoureux\n",
    "    * PREP + G très abandonné\n",
    "    * PREP + V très aimé\n",
    "    * PREP + J très étonnant\n",
    "    * PREP + SS très ils\n",
    "    * PREP + A très étonnamment\n",
    "    * PREP + p très attendu\n",
    "    * PREP + di très autre\n",
    "    * PREP + rc très au\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Partie 1\n",
    "*chaque phrase est prise individuellement,\n",
    "    * découpée en blocs,\n",
    "        * qui sont chacuns trimés si ce sont des mots\n",
    "        * s'il y a plusieurs mots dans le bloc, ils sont séparés\n",
    "    + Partie 2\n",
    "    * pour chaque couple de mots\n",
    "        * si la liaison est possible,\n",
    "            * et qu'elle est obligatoire, l'api avec la liaison est généré\n",
    "            * et qu'elle est facultative,\n",
    "                * si l'utilisateur l'a choisi, l'api avec la liaison est généré\n",
    "                * sinon l'api sans la liaison est généré\n",
    "\n",
    "        + Partie 3\n",
    "        * si la liaison n'est pas possible,\n",
    "            * si le mot est dans bdlex, l'api est généré\n",
    "            * sinon le mot est laissé tel quel (il a déjà les étoiles)        \n",
    "\n",
    "    * pour le dernier mot de la phrase, \n",
    "        * si le mot est dans bdlex, l'api est généré\n",
    "        * sinon le mot est laissé tel quel (il a déjà les étoiles) \n",
    "\n",
    "+ Partie 4\n",
    "* le message à l'utilisateur et la phrase en api est imprimée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparer structures Tours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "altEspace=r\"\\s|_\"\n",
    "connecteurs=[\n",
    "    u\"et\", u\"alors\", u\"du coup\", u\"sinon\", u\"par contre\", u\"ça veut dire\", u\"enfin\",\n",
    "u\"après\", u\"donc\", u\"puisque\", u\"puisqu'\", u\"en fait\", u\"mais\", u\"parce que\", u\"parce qu'\", u\"même si\" , u\"d'abord\", u\"et puis\"\n",
    "]\n",
    "immigration=[u\"cible\",u\"frontières\",u\"accueillir\",u\"démonstrations\",u\"accompagner\",u\"contre-courant\",u\"importantes\",u\"droit de bouger\",u\"peur\",u\"solution\",u\"durable\",u\"refuge\",u\"assassins\",u\"tueurs\",u\"violeurs\",u\"fuient\",u\"misère\",u\"chaos\",u\"circulation\",u\"liberté\",u\"installation\",u\"solidarité\",u\"migrants\",u\"ouverture\",u\"frontières\",u\"conditions\",u\"réflexe\",u\"normal\",u\"sain\",u\"accueillir\",u\"moyens\",u\"intégrés\",u\"flux\",u\"migratoires\",u\"énormes\",u\"moyens\",u\"structures\",u\"loger\",u\"répondre\",u\"peuples\",u\"coopération\",u\"indéniable\",u\"réfugiés\",u\"digne\",u\"prendre la main\",u\"équilibrée\",u\"accueil\",u\"correctement\",u\"favorable\",u\"hébergés\",u\"mise à l’abri\",u\"centre humanitaire\",u\"situation\",u\"convenable\",u\"heurtée\",u\"valeurs\",u\"république\",u\"comportement\",u\"politique\",u\"associations\",u\"hébergement\",u\"respect\",u\"lien\",u\"confiance\",u\"travailleur\",u\"social\",u\"policiers\",u\"gazent\",u\"détruisent\",u\"nourriture\",u\"refusent\",u\"points d’eau\",u\"négation\",u\"partent\",u\"parcours\",u\"migration\",u\"terribles\",u\"touche\",u\"cimetière\",u\"choquant\",u\"scandaleux\",u\"dignité\",u\"impression\",u\"responsabilités\",u\"multiplication\",u\"conflits\",u\"excessivement\",u\"violents\",u\"solution\",u\"seule\",u\"quitter\",u\"victimes\",u\"esclavage\",u\"regression\",u\"humanité\",u\"guerre\",u\"état\",u\"pauvreté\",u\"pays\",u\"conditions\",u\"respect\",u\"organisme\",u\"ficher\",u\"contrôler\",u\"demande\",u\"papiers\",u\"droits\",u\"échec\",u\"aide\",u\"développement\",u\"vivre\",u\"famille\",u\"terrain\",u\"voir\",u\"souffrance\",u\"regression\",u\"France\",u\"richesse\",u\"humanité\",u\"extraordinaire\",u\"intérger\",u\"asile\",u\"question\",u\"temporaire\",u\"débat\",u\"souhaitons\",u\"propositions\",u\"européen\",u\"rapprocher\",u\"proches\",u\"position\",u\"absurdité\",u\"ONG\",u\"aider\",u\"détresse\",u\"lois\",u\"règles\",u\"principes\",u\"humanité\",u\"vulnérable\",u\"enfants\",u\"sauver\",u\"vies\",u\"générosité\",u\"classes\",u\"francophones\",u\"découverte\",u\"langue\",u\"logements\",u\"associations\",u\"continuerai\",u\"nouveaux\",u\"clé\",u\"chômage\",u\"héberger\",u\"employer\",u\"former\",u\"éduquer\",u\"milliers\",u\"vaincre\",u\"réticences\",u\"acccepter\",u\"aménage\",u\"refuges\",u\"régions\",u\"viennent\",u\"zones\",u\"protégées\",u\"normales\",u\"chez-eux\",u\"circuler\",u\"librement\",u\"Allemagne\",u\"obligations\",u\"humanitaire\",u\"préserver\",u\"populations\",u\"bataille\",u\"hégémonique\",u\"bataille\",u\"s'installer\",u\"mois\",u\"désaccord\",u\"moins\",u\"accueillant\",u\"bataille\",u\"idéologique\",u\"pression\",u\"chercher\",u\"immigrés\",u\"de force\",u\"délogés\",u\"expulsés\",u\"xénophobie\",u\"racistes\",u\"recul\",u\"idées\",u\"argent\",u\"dramatique\",u\"problèmes\",u\"économiques\",u\"interventions\",u\"militaires\",u\"famine\",u\"massacres\",u\"répression\",u\"bombardé\",u\"manqué\",u\"devoir\",u\"indigne\",u\"déposer\",u\"demande d'asile\",u\"absurde\",u\"campements\",u\"regrette\",u\"dispositifs\",u\"pérennes\",u\"divergence\",u\"appréciation\",u\"reconstitution\",u\"campements\",u\"crise\",u\"emballement\",u\"otage\",u\"noyer\",u\"guerre\",u\"bombardement\",u\"morts\",u\"difficultés\",u\"blocage\",u\"maltraiter\",u\"seul\",u\"privation\",u\"absurde\",u\"illusion\",u\"mensonge\",u\"ghettos\",u\"français\",u\"divisés\",u\"meurent\",u\"fuient\",u\"devoirs\",u\"moraux\",u\"centres de rétention\",u\"protection\",u\"cause\",u\"guerre\",u\"lieu\",u\"couteux\",u\"charge\",u\"services\",u\"gratuits\",u\"système\",u\"gestion\",u\"patrimoine\",u\"nationale\",u\"importante\",u\"fossé\",u\"travail\",u\"difficulté\",u\"maîtrise\",u\"suspendre\",u\"shengen\",u\"arrêter\",u\"immigration\",u\"communautarisme\",u\"fondamentalisme\",u\"islamiste\",u\"gel\",u\"constructions\",u\"mosquées\",u\"islam\",u\"radicaux\",u\"vivier\",u\"radicalisation\",u\"soldats\",u\"djihadisme\",u\"terroristes\",u\"puissance\",u\"terreau\",u\"frappent\",u\"clandestine\",u\"massive\",u\"insécurité\",u\"terrifiante\",u\"forces de l'ordre\",u\"mesures législatives\",u\"suppression\",u\"enrayer\",u\"processus\",u\"récupérer\",u\"clandestins\",u\"absence\",u\"bateaux\",u\"en mer\",u\"ramener\",u\"facilite\",u\"réseaux\",u\"criminels\",u\"laisser\",u\"mourir\",u\"sauver\",u\"ramener\",u\"port d’origine\",u\"corrompus\",u\"passeurs\",u\"arme\",u\"catastrophique\",u\"occupation\",u\"illégale\",u\"inexpulsable\",u\"engrenage\",u\"procédures\",u\"demandeurs\",u\"déboutés\",u\"économique\",u\"raisons\",u\"traverser\",u\"sortis\",u\"territoire\",u\"arrivés\",u\"origine\",u\"critères\",u\"extrêmement\",u\"incitatif\",u\"centres\",u\"AME\",u\"CMU\",u\"ensemble\",u\"folle\",u\"coût\",u\"faramineux\",u\"malvenu\",u\"opposition\",u\"opposée\",u\"communautarisme\",u\"fondamentalisme\",u\"imposer\",u\"ennemi\",u\"contre\",u\"rejet\",u\"anti-démocratique\",u\"enfermemement\",u\"séparation\",u\"pourissement\",u\"étranger\",u\"drame\",u\"faillite\",u\"déloyale\",u\"ménage\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "immigrationMots=sorted(set(immigration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compterVoyelles(chaine):\n",
    "    result=0\n",
    "    for element in chaine:\n",
    "        if element in voyelles:\n",
    "            result+=1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lister les groupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def listerGroupes(repRacine):\n",
    "    return glob.glob(repRacine+r\"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chercher les inconnus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dicterInconnus(lInconnus):\n",
    "    result={}\n",
    "    for line in lInconnus:\n",
    "        line=line.strip()\n",
    "        p=line.split(\";\")\n",
    "        if len(p[1])!=0:\n",
    "            if len(p)>8:\n",
    "                print (p)\n",
    "                for i in range(len(p)+1,7):\n",
    "                    p.append(\"\")\n",
    "            result[p[0].lower()]=(p[0],p[1],p[2],p[3],p[4],p[5],p[6],p[7],p[8])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lireInconnus(repGroupe):\n",
    "    result={}\n",
    "    fInconnus=repGroupe+\"/inconnus.txt\"\n",
    "    try:\n",
    "        with codecs.open(fInconnus,\"r\",encoding=\"utf8\") as inFile:\n",
    "            lInconnus=inFile.readlines()\n",
    "        result=dicterInconnus(lInconnus)\n",
    "        print (result)\n",
    "    except:\n",
    "        print (\"pas d'inconnus.txt\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lister les EAFs du groupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def listerEAFs(repGroupe):\n",
    "    result=[f for f in glob.glob(repGroupe+r\"/*.eaf\") if \"phonetique\" not in unidecode.unidecode(f.lower())]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lister les mots d'un EAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trimer(mot):\n",
    "#     mot=mot.lower()\n",
    "    for p in u',;.-?!“”‘’‛‟′″´˝\"«»':      # Modifié le 12/01/20 pour gérer les deux points comme marqueur dans les mots\n",
    "        mot=mot.replace(p, ' ')\n",
    "    mot=mot.strip()\n",
    "    return mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def deparentheser(mot):\n",
    "    forme=mot\n",
    "    graphie=mot\n",
    "    m=re.search(r\"\\([^)]*\\)\",forme)\n",
    "    while (m):\n",
    "        forme=re.sub(r\"\\(([^)]*)\\)\",\"\\g<1>\", forme)\n",
    "        graphie=re.sub(r\"\\(([\\wâàéèêëîïôùûüçÂÀÉÈÊËÎÏÔÙÛÜÇæœÆŒ]+['’]?)\\)\",\"'\", graphie)\n",
    "        m=re.search(r\"\\([^)]*\\)\",forme)\n",
    "    forme=forme.lstrip(\")\").rstrip(\"(\")\n",
    "    forme=forme.replace(\":\",\"\")\n",
    "    if graphie.endswith(\"(\"):\n",
    "        graphie=graphie[:-1]+\"'\"\n",
    "    return(forme,graphie)\n",
    "#    motsAbreges[mot]={\"lexical\":forme, \"graphie\":graphie}    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bec ter', ('Maintenant', \"Main'nant\"))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimer(\"bec‘ter\"), deparentheser(\"Main(te)nant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenizerTour(tour,lexique):\n",
    "    listeMots=[]\n",
    "    listeTokens=[]\n",
    "    elements=re.findall(r\"[\\wâàéèêëîïôùûüçÂÀÉÈÊËÎÏÔÙÛÜÇæœÆŒ():]+|[-.…,—–()\\[\\]\\/#\\\"“”‘«»<>'’=~]| ?[;!?:]\", tour)\n",
    "    elements=[x for x in elements if x!=u\" \"]\n",
    "    mots=[x for x in elements if not x in u\"-_.…,—–()\\[\\]\\/#\\\"“”‘«»<>'’=~:\" and not x in [u\" ;\",u\" !\",u\" ?\",u\" :\"]]\n",
    "    for element in elements:\n",
    "        if element in mots:\n",
    "            mot = trimer(element)\n",
    "            (forme,graphie)=deparentheser(mot)\n",
    "            listeMots.append(graphie)\n",
    "            if forme.lower() in lexique:\n",
    "                listeTokens.append(lexique[forme.lower()])\n",
    "            else:\n",
    "                listeTokens.append(forme.lower())\n",
    "        else:\n",
    "            listeMots.append(element)\n",
    "            listeTokens.append(element)\n",
    "    return listeMots,listeTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenizerTexte(texte):\n",
    "    listeMots=set()\n",
    "    elements=re.findall(r\"[\\wâàéèêëîïôùûüçÂÀÉÈÊËÎÏÔÙÛÜÇæœÆŒ():]+|[-.…,—–()\\[\\]\\/#\\\"“”‘«»<>'’=~]| ?[;!?:]\", texte)\n",
    "    elements=[x for x in elements if x!=u\" \"]\n",
    "    mots=[x for x in elements if not x in u\"-_.…,—–()\\[\\]\\/#\\\"“”‘«»<>'’=~:\" and not x in [u\" ;\",u\" !\",u\" ?\",u\" :\"]]\n",
    "    for mot in mots:\n",
    "        mot = trimer(mot)\n",
    "        (forme,graphie)=deparentheser(mot)\n",
    "        listeMots.add(forme.lower())\n",
    "    return list(listeMots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def listerMots(nomEAF):\n",
    "    xmlEAF=ET.parse(nomEAF,parser)\n",
    "    texteLignesEAF=[]\n",
    "    for tier in xmlEAF.xpath(\"//TIER\"):\n",
    "        for annotation in tier.xpath(\"ANNOTATION/ALIGNABLE_ANNOTATION\"):\n",
    "            aValue=annotation.xpath(\"ANNOTATION_VALUE/text()\")\n",
    "            texteLignesEAF.append(\"\\n\".join(aValue))\n",
    "    texteEAF=\"\\n\".join(texteLignesEAF)\n",
    "    return tokenizerTexte(texteEAF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def faireLexique(nomEAF,groupeInconnus):\n",
    "    extraitBdlexique={}\n",
    "    listeMots=listerMots(nomEAF)\n",
    "    for forme in listeMots:\n",
    "        if forme in groupeInconnus:\n",
    "            extraitBdlexique[forme]=groupeInconnus[forme]\n",
    "        elif forme in bdlexique:\n",
    "            extraitBdlexique[forme]=bdlexique[forme]\n",
    "        else:\n",
    "            groupeInconnus[forme]=(forme,\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\")\n",
    "            extraitBdlexique[forme]=(forme,\"***%s***\"%forme,\"\",\"\",\"\",\"\",\"\",\"\",\"\")\n",
    "    return extraitBdlexique,groupeInconnus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dicterContent(content,lexique):\n",
    "    lMots,lTokens=tokenizerTour(content,lexique)\n",
    "    result=[[mot,lTokens[nMot]] for nMot,mot in enumerate(lMots)]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changement de convention d'annotation pour les disfluences\n",
    "- 2021: \n",
    "    - [blablabla] => \\<disfluence\\>blablabla\\</disfluence\\>\n",
    "- 2022: \n",
    "    - [rire] => \\<rire/\\>\n",
    "    - {blablabla} => \\<disfluence\\>blablabla\\</disfluence\\>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def makeAttribs(chaine):\n",
    "    result=chaine\n",
    "    mAttribs=re.match(r\"(\\w+) (((\\w+)=[\\\"\\']?[^\\\"\\']+[\\\"\\']?\\s*)+)\",chaine)\n",
    "    if mAttribs:\n",
    "        result=mAttribs.group(1)+\" \"+mAttribs.group(2)\n",
    "    else:\n",
    "        mType=re.match(r\"(\\w+) (.*)\",chaine)\n",
    "        if mType:\n",
    "            result=mType.group(1)+\" type='%s'\"%mType.group(2)\n",
    "    return result \n",
    "\n",
    "def makeTagAttribs(chaine):\n",
    "    resTag=unidecode.unidecode(chaine)\n",
    "    resAttrib={}\n",
    "    mAttribs=re.match(r\"(\\w+) (((\\w+)=[\\\"\\']?[^\\\"\\']+[\\\"\\']?\\s*)+)\",chaine,re.U)\n",
    "    if mAttribs:\n",
    "#         print (mAttribs.groups())\n",
    "        resTag=unidecode.unidecode(mAttribs.group(1))\n",
    "        if \"'\" not in mAttribs.group(2) and '\"' not in mAttribs.group(2):\n",
    "            resAttrib=re.findall(r\"(\\w+)=(\\S+)\",mAttribs.group(2),re.U)\n",
    "#             print (resAttrib)\n",
    "            resAttrib={unidecode.unidecode(k):v for k,v in resAttrib}            \n",
    "        else:\n",
    "            resAttrib=re.findall(r\"(\\w+)=[\\\"\\']([^\\\"\\']+)[\\\"\\']\",mAttribs.group(2),re.U)\n",
    "#             print (resAttrib)\n",
    "            resAttrib={unidecode.unidecode(k):v for k,v in resAttrib}\n",
    "    else:\n",
    "        mType=re.match(r\"(\\w+) (.*)\",chaine,re.U)\n",
    "        if mType:\n",
    "            resTag=unidecode.unidecode(mType.group(1))\n",
    "            resAttrib={\"type\":mType.group(2)}\n",
    "    return resTag,resAttrib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e t=acc p=fRãsE'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "makeAttribs(u\"e t=acc p=fRãsE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def baliserTour(tour,lexique):    \n",
    "    lMorceaux=re.split(r\"(\\[[^]]*\\])\",tour)\n",
    "#     print lMorceaux\n",
    "    newTour=[]\n",
    "    for lMorceau in lMorceaux:\n",
    "        m1=re.match(r\"\\[(.*)\\]\",lMorceau)\n",
    "        m2=re.match(r\"\\{(.*)\\}\",lMorceau)\n",
    "        if m1:\n",
    "            bContent=m1.group(1)\n",
    "            if bContent.startswith(u\"=\"):\n",
    "                newTour.append([\"[%s]\"%bContent,\"<%s>\"%makeAttribs(bContent[1:])])\n",
    "            elif bContent.startswith(u\"/\"):\n",
    "                newTour.append([\"[%s]\"%bContent,\"</%s>\"%bContent[1:]])\n",
    "            else:\n",
    "                newTour.append([\"[%s]\"%bContent,\"<%s/>\"%makeAttribs(bContent)])\n",
    "        elif m2:\n",
    "            aContent=m2.group(1)\n",
    "            newTour.append([\"{\",\"<disfluence>\"])\n",
    "            aContentDict=dicterContent(aContent,lexique)\n",
    "            newTour.extend(aContentDict)\n",
    "            newTour.append([\"}\",\"</disfluence>\"])\n",
    "\n",
    "        else:\n",
    "            if lMorceau.strip()!=\"\":\n",
    "                lMorceauDict=dicterContent(lMorceau.strip(),lexique)\n",
    "                newTour.extend(lMorceauDict)\n",
    "    return newTour"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testBaliseTour=baliserTour(u\"qu’est ce c’est ça [=CH] cette affaire Dupont là [/CH]\",subBdlexique)\n",
    "print (testBaliseTour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trouverMot2(nMot1,tourBalises):\n",
    "    result=None\n",
    "    if len(tourBalises[nMot1])>1:\n",
    "#         print (tourBalises[nMot1][1])\n",
    "        if isinstance(tourBalises[nMot1][1],tuple):\n",
    "            for nMot2,mot2 in enumerate(tourBalises[nMot1+1:]):\n",
    "                if len(mot2)>1:\n",
    "#                     print (mot2[0],mot2[1])\n",
    "                    if isinstance(mot2[1],tuple):\n",
    "                        result=nMot1+1+nMot2\n",
    "                        break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def faireTourPhon(tourBalises):\n",
    "    result=[]\n",
    "    for nMot1 in range(len(tourBalises)):\n",
    "        nMot2=trouverMot2(nMot1,tourBalises)\n",
    "        mot1=tourBalises[nMot1]\n",
    "#         print (mot1)\n",
    "        if nMot2:\n",
    "            mot2=tourBalises[nMot2]\n",
    "            contexteLiaison=liaison_obligatoire(mot1,mot2)\n",
    "#             print (nMot1,mot1[1],contexteLiaison)\n",
    "            result.append(formerLiaison(mot1,mot2,contexteLiaison))\n",
    "        else:\n",
    "#             print (mot1)\n",
    "            if len(mot1)==2 and isinstance(mot1[1],tuple):\n",
    "                result.append(sampa2api(mot1[1][1]))\n",
    "            elif len(mot1)==2:\n",
    "#                 print (\"len(mot1)==2\", mot1)\n",
    "                result.append(mot1[1])\n",
    "            else:\n",
    "#                 print (\"len(mot1)!=2\", mot1)\n",
    "                result.append(mot1[0])\n",
    "    return \"\".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transcrireTourEAF(tour,lexique):\n",
    "    tourBalises=baliserTour(tour,lexique)\n",
    "    tourEAF=faireTourPhon(tourBalises)\n",
    "    return tourEAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transcrireEAF(nomEAF,subBdlexique):\n",
    "    xmlEAF=ET.parse(nomEAF,parser)\n",
    "    texteLignesEAF=[]\n",
    "    for nAnnotation,annotation in enumerate(xmlEAF.xpath(\"//ANNOTATION_VALUE\")):\n",
    "        if annotation.text:\n",
    "            tourEAF=transcrireTourEAF(annotation.text,subBdlexique)\n",
    "            annotation.text=tourEAF\n",
    "    return xmlEAF\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def enchasseMot(balise,mot,graphie,phono,nMot,nTour):\n",
    "    ortho=mot[0]\n",
    "    cat=mot[3]\n",
    "    if cat in [u\"J\",u\"K\"]:\n",
    "        cat=u\"Adj\"\n",
    "    ms=mot[4]\n",
    "    vs=mot[5]\n",
    "    lexeme=mot[6].upper()\n",
    "    freq=mot[8]\n",
    "    nbVoyelles=compterVoyelles(mot[1])\n",
    "    if u\" \" in vs:\n",
    "        vs=u\"\"\n",
    "    motAttributs={\"cat\":cat,\"ms\":ms,\"vs\":vs,\"phon\":phono,\"nbsyll\":str(nbVoyelles),\"ortho\":ortho, \"lexeme\":lexeme, \"freq\":freq, \"id\":\"%05d%03d\"%(nTour,nMot)}\n",
    "    baliseMotBDL=ET.SubElement(balise,\"motBDL\",motAttributs)\n",
    "    baliseMotBDL.text=graphie\n",
    "    return nbVoyelles\n",
    "    \n",
    "\n",
    "def enchasseNonMot(balise,nonMot):\n",
    "    lNonMot=ET.SubElement(balise, \"punct\")\n",
    "    lNonMot.text=nonMot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "mapBalises={\"???\":\"incomprehensible\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def enchasserTourTRS(tour,tourMots):\n",
    "    nbVoyelles=0\n",
    "    nbMots=0\n",
    "    lConnecteurs=set()\n",
    "    lThemeMots=set()\n",
    "#     print (tourMots)\n",
    "#     print (tour)\n",
    "\n",
    "    nTour=int(tour.attrib[\"id\"])\n",
    "    stackBalises=[tour]\n",
    "    for nMot1,mot1 in enumerate(tourMots):\n",
    "#         print (nMot1,mot1)\n",
    "        graphie=mot1[0]\n",
    "        lexMot=mot1[1]\n",
    "        if lexMot:\n",
    "            orthoMot=lexMot[0]\n",
    "        else:\n",
    "            orthoMot=\"\"\n",
    "        if orthoMot in connecteurs:\n",
    "            lConnecteurs.add(orthoMot)\n",
    "        if orthoMot in lThemeMots:\n",
    "            lThemeMots.add(orthoMot)\n",
    "        if isinstance(lexMot,tuple):\n",
    "            nbMots+=1\n",
    "            nMot2=trouverMot2(nMot1,tourMots)\n",
    "            phono=\"???\"\n",
    "            if nMot2:\n",
    "                mot2=tourMots[nMot2]\n",
    "                contexteLiaison=liaison_obligatoire(mot1,mot2)\n",
    "#                 print (nMot1,mot1[1],contexteLiaison)\n",
    "                phono=formerLiaison(mot1,mot2,contexteLiaison)\n",
    "                \n",
    "            nbVoyelles+=enchasseMot(stackBalises[-1],lexMot,graphie,phono,nMot1,nTour)\n",
    "        else:\n",
    "            m=re.match(r\"</.*>\",lexMot)\n",
    "            if m:\n",
    "                stackBalises.pop(-1)\n",
    "#                 print (\"pop\",groupeEAF)\n",
    "#                 print (tourMots)\n",
    "#                 print (stackBalises[-1].tag)\n",
    "            else:\n",
    "                m=re.match(r\"<(.*[^/])/?>\",lexMot)\n",
    "                if m:\n",
    "                    locBalise=m.group(1)\n",
    "#                     print (\"stackBalises\",ET.tostring(stackBalises[-1]),locBalise)\n",
    "                    if locBalise in mapBalises:\n",
    "                        locBalise=mapBalises[locBalise]\n",
    "                    locTag,locAttribs=makeTagAttribs(locBalise)\n",
    "                    newBalise=ET.SubElement(stackBalises[-1],locTag,locAttribs)\n",
    "                    stackBalises.append(newBalise)\n",
    "#                     print (\"push\",stackBalises[-1].tag)\n",
    "                else:\n",
    "                    enchasseNonMot(stackBalises[-1],lexMot)\n",
    "    tour.attrib[\"nbSyll\"]=str(nbVoyelles)\n",
    "    tour.attrib[\"nbMots\"]=str(nbMots)\n",
    "    if lConnecteurs:\n",
    "        tour.attrib[\"connecteurs\"]=\" \".join(lConnecteurs)\n",
    "    if lThemeMots:\n",
    "        tour.attrib[\"themeMots\"]=\" \".join(lThemeMots)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<Turn>\\n  <tour id=\"00001\">\\n    <mot>mot1</mot>\\n    <nom>\\n      <mot>nom1</mot>\\n    </nom>\\n    <mot>mot2</mot>\\n  </tour>\\n</Turn>\\n'\n"
     ]
    }
   ],
   "source": [
    "turn=ET.Element(\"Turn\")\n",
    "tour=ET.SubElement(turn,\"tour\",id=\"00001\")\n",
    "e1=ET.SubElement(tour,\"mot\")\n",
    "e1.text=\"mot1\"\n",
    "nom1=ET.SubElement(tour,\"nom\")\n",
    "nom1e1=ET.SubElement(nom1,\"mot\")\n",
    "nom1e1.text=\"nom1\"\n",
    "e2=ET.SubElement(tour,\"mot\")\n",
    "e2.text=\"mot2\"\n",
    "print (ET.tostring(turn,pretty_print=True,encoding=\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transcrireTRS(nomEAF,lexique):\n",
    "    xmlEAF=ET.parse(nomEAF,parser)\n",
    "\n",
    "    #################\n",
    "    #\n",
    "    # Récupération des timestamps\n",
    "    #\n",
    "    #################\n",
    "    \n",
    "    ts={}\n",
    "    for timeOrder in xmlEAF.xpath(\"//TIME_ORDER/TIME_SLOT\"):\n",
    "        tsID=timeOrder.attrib[\"TIME_SLOT_ID\"]\n",
    "        tsTime=timeOrder.attrib[\"TIME_VALUE\"]\n",
    "        ts[tsID]=tsTime\n",
    "    tiersTypes={}\n",
    "    annotations={}\n",
    "    turnTextes=[]\n",
    "\n",
    "    #################\n",
    "    #\n",
    "    # Récupération des tours\n",
    "    #\n",
    "    #################\n",
    "    \n",
    "    for tier in xmlEAF.xpath(\"//TIER\"):\n",
    "        tierID=tier.attrib[\"TIER_ID\"]\n",
    "        tierType=tier.attrib[\"LINGUISTIC_TYPE_REF\"]\n",
    "        tiersTypes[tierID]=tierType\n",
    "        for annotation in tier.xpath(\"ANNOTATION/ALIGNABLE_ANNOTATION\"):\n",
    "            aID=annotation.attrib[\"ANNOTATION_ID\"]\n",
    "            aTS1=annotation.attrib[\"TIME_SLOT_REF1\"]\n",
    "            aTS2=annotation.attrib[\"TIME_SLOT_REF2\"]\n",
    "            aBegin=time2TRS(ts[aTS1])\n",
    "            aEnd=time2TRS(ts[aTS2])\n",
    "            aValue=annotation.xpath(\"ANNOTATION_VALUE/text()\")\n",
    "            turnTextes.append(\"\\n\".join(aValue))\n",
    "            turn=(tierID, aBegin, aEnd, aValue)\n",
    "            annotations[aID]=turn\n",
    "            \n",
    "\n",
    "    #################\n",
    "    #\n",
    "    # Fabrication entête TRS\n",
    "    #\n",
    "    #################\n",
    "\n",
    "    root=ET.Element(\"Trans\")\n",
    "    speakers=ET.SubElement(root, \"Speakers\")\n",
    "    speakerID={}\n",
    "    for nSpk,spk in enumerate(tiersTypes):\n",
    "        ET.SubElement(speakers,\"Speaker\",id=\"spk%d\"%(nSpk+1),name=spk)\n",
    "        speakerID[spk]=\"spk%d\"%(nSpk+1)\n",
    "\n",
    "    episode=ET.SubElement(root, \"Episode\")\n",
    "    section=ET.SubElement(episode, \"Section\")   \n",
    "    tourId=0\n",
    "    for k,v in sorted(annotations.items(),key=lambda x: int(x[0].strip(\"a\"))):\n",
    "        #\n",
    "        # créer la structure Turn avec speaker, startTime, endTime\n",
    "        # et tour avec id\n",
    "        #\n",
    "        turn=ET.SubElement(section,\"Turn\",speaker=speakerID[v[0]],startTime=v[1],\\\n",
    "                                          endTime=v[2])\n",
    "        tour=ET.SubElement(turn,\"tour\",id=\"%05d\"%tourId)\n",
    "        #\n",
    "        # découper le texte de l'annotation\n",
    "        #\n",
    "        vTextes=baliserTour(\"\\n\".join(v[3]),lexique)\n",
    "        if debug: print (vTextes)\n",
    "        enchasserTourTRS(tour,vTextes)\n",
    "        tourId+=1\n",
    "    tree = ET.ElementTree(root)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ecrireInconnus(dInconnus,repGroupe):\n",
    "    fInconnus=repGroupe+\"/inconnus.txt\"\n",
    "    with codecs.open(fInconnus,\"w\",encoding=\"utf8\") as outFile:\n",
    "        for inconnu in dInconnus.values():\n",
    "            outFile.write(\";\".join(inconnu)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Test/AAA']\n"
     ]
    }
   ],
   "source": [
    "repRacine=\"/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-20%d/0-Test/\"%annee\n",
    "repGroupes=listerGroupes(repRacine)\n",
    "print (repGroupes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Test/AAA\n",
      "['roh', 'Ro', '', '', '', '', '', '', '']\n",
      "['all', 'ol', '', '', '', '', '', '', '']\n",
      "['hyper', 'ipER', '', '', '', '', '', '', '']\n",
      "['fiote', 'fjot', '', '', '', '', '', '', '']\n",
      "['léadrien', 'leadRiê', '', '', '', '', '', '', '']\n",
      "['ét', 'et', '', '', '', '', '', '', '']\n",
      "['damso', 'damso', '', '', '', '', '', '', '']\n",
      "['marion', 'marjô', '', '', '', '', '', '', '']\n",
      "['thanks', 'Tâks', '', '', '', '', '', '', '']\n",
      "['cuts', 'k9t', '', '', '', '', '', '', '']\n",
      "['ttc', 'tetese', '', '', '', '', '', '', '']\n",
      "['alt', 'alt', '', '', '', '', '', '', '']\n",
      "['suç', 'sys', '', '', '', '', '', '', '']\n",
      "['larrouy', 'laRwi', '', '', '', '', '', '', '']\n",
      "['cut', 'k9t', '', '', '', '', '', '', '']\n",
      "['quee', 'kwi', '', '', '', '', '', '', '']\n",
      "['21', 'vîteî', '', '', '', '', '', '', '']\n",
      "['royan', 'Rwajâ', '', '', '', '', '', '', '']\n",
      "['2019', 'd2mildizn9f', '', '', '', '', '', '', '']\n",
      "['2018', 'd2mildizHit', '', '', '', '', '', '', '']\n",
      "['éro', 'eRo', '', '', '', '', '', '', '']\n",
      "['har', 'aR', '', '', '', '', '', '', '']\n",
      "['friendly', 'fREndli', '', '', '', '', '', '', '']\n",
      "['woh', 'wo', '', '', '', '', '', '', '']\n",
      "['boris', 'boRis', '', '', '', '', '', '', '']\n",
      "['monica', 'monika', '', '', '', '', '', '', '']\n",
      "['sociétale', 'sosjetal', '', '', '', '', '', '', '']\n",
      "['montmoreau', 'mômoRo', '', '', '', '', '', '', '']\n",
      "['very', 'veRi', '', '', '', '', '', '', '']\n",
      "['sexualit', 'sEksHalit', '', '', '', '', '', '', '']\n",
      "['bear', 'bER', '', '', '', '', '', '', '']\n",
      "['geek', 'gik', '', '', '', '', '', '', '']\n",
      "['pansexuel', 'pâseksHEl', '', '', '', '', '', '', '']\n",
      "['not', 'nOt', '', '', '', '', '', '', '']\n",
      "['elena', 'elena', '', '', '', '', '', '', '']\n",
      "['ohlala', 'olala', '', '', '', '', '', '', '']\n",
      "['day', 'dEj', '', '', '', '', '', '', '']\n",
      "['cringy', 'krinZi', '', '', '', '', '', '', '']\n",
      "['gray', 'gRE', '', '', '', '', '', '', '']\n",
      "['trash', 'tRaS', '', '', '', '', '', '', '']\n",
      "['qu', 'k', '', '', '', '', '', '', '']\n",
      "['ç', 's', '', '', '', '', '', '', '']\n",
      "['james', 'ZEms', '', '', '', '', '', '', '']\n",
      "['50', 'sîkât', '', '', '', '', '', '', '']\n",
      "['garcia', 'gaRsia', '', '', '', '', '', '', '']\n",
      "['louis', 'lwi', '', '', '', '', '', '', '']\n",
      "['chev', 'S@v', '', '', '', '', '', '', '']\n",
      "['el', 'El', '', '', '', '', '', '', '']\n",
      "['ouh', 'u', '', '', '', '', '', '', '']\n",
      "['instagram', 'êstagRam', '', '', '', '', '', '', '']\n",
      "['fair', 'fEr', '', '', '', '', '', '', '']\n",
      "['ouhouhouh', 'u.u.u', '', '', '', '', '', '', '']\n",
      "['queer', 'kwir', '', '', '', '', '', '', '']\n",
      "['sex', 'sEks', '', '', '', '', '', '', '']\n",
      "['auteure', 'ot9r', '', '', '', '', '', '', '']\n",
      "['arg', 'aRg', '', '', '', '', '', '', '']\n",
      "['mélo', 'melo', '', '', '', '', '', '', '']\n",
      "['podcasts', 'pOdkast', '', '', '', '', '', '', '']\n",
      "['yes', 'jEs', '', '', '', '', '', '', '']\n",
      "['gé', 'Ze', '', '', '', '', '', '', '']\n",
      "['hypersexuels', 'ipERseksHEl', '', '', '', '', '', '', '']\n",
      "['butch', 'b9tS', '', '', '', '', '', '', '']\n",
      "['grossophobie', 'gRosofobi', '', '', '', '', '', '', '']\n",
      "['laserquest', 'lazERkwEst', '', '', '', '', '', '', '']\n",
      "['rer', 'ER2ER', '', '', '', '', '', '', '']\n",
      "['quelqu', 'kElk', '', '', '', '', '', '', '']\n",
      "['raclement', 'Rakl@mâ', '', '', '', '', '', '', '']\n",
      "['re', 'R@', '', '', '', '', '', '', '']\n",
      "['3', 'tRwa', '', '', '', '', '', '', '']\n",
      "['sml', 'EsEMEl', '', '', '', '', '', '', '']\n",
      "['diaz', 'diaz', '', '', '', '', '', '', '']\n",
      "['melo', 'melo', '', '', '', '', '', '', '']\n",
      "['7', 'sEt', '', '', '', '', '', '', '']\n",
      "['import', 'îpoRt', '', '', '', '', '', '', '']\n",
      "['ordr', 'oRdR', '', '', '', '', '', '', '']\n",
      "['couilles', 'kuj', '', '', '', '', '', '', '']\n",
      "['sexuali', 'seksHali', '', '', '', '', '', '', '']\n",
      "['pun', 'p9n', '', '', '', '', '', '', '']\n",
      "['christophe', 'kristof', '', '', '', '', '', '', '']\n",
      "['men', 'mEn', '', '', '', '', '', '', '']\n",
      "['hypersexuelle', 'ipERseksHEl', '', '', '', '', '', '', '']\n",
      "['meh', 'mE', '', '', '', '', '', '', '']\n",
      "['crédo', 'kRedo', '', '', '', '', '', '', '']\n",
      "['bi', 'bi', '', '', '', '', '', '', '']\n",
      "['lgbt', 'ElZebete', '', '', '', '', '', '', '']\n",
      "['disneyland', 'disnelâd', '', '', '', '', '', '', '']\n",
      "['céline', 'selin', '', '', '', '', '', '', '']\n",
      "['thank', 'fEnk', '', '', '', '', '', '', '']\n",
      "['hétéro', 'eteRo', '', '', '', '', '', '', '']\n",
      "['oro', 'oRo', '', '', '', '', '', '', '']\n",
      "['éta', 'eta', '', '', '', '', '', '', '']\n",
      "['émifion', 'emifjô', '', '', '', '', '', '', '']\n",
      "['sexu', 'seksy', '', '', '', '', '', '', '']\n",
      "['jou', 'Zu', '', '', '', '', '', '', '']\n",
      "['co', 'ko', '', '', '', '', '', '', '']\n",
      "['hypersexuel', 'ipERseksyEl', '', '', '', '', '', '', '']\n",
      "['pénétratif', 'penetRatif', '', '', '', '', '', '', '']\n",
      "['ca', 'ka', '', '', '', '', '', '', '']\n",
      "['rachel', 'RaSEl', '', '', '', '', '', '', '']\n",
      "['thierry', 'tieRi', '', '', '', '', '', '', '']\n",
      "['julia', 'Zylia', '', '', '', '', '', '', '']\n",
      "['one', 'wan', '', '', '', '', '', '', '']\n",
      "['miranda', 'miRâda', '', '', '', '', '', '', '']\n",
      "['scèn', 'sEn', '', '', '', '', '', '', '']\n",
      "['normer', 'nORme', '', '', '', '', '', '', '']\n",
      "['anti', 'âti', '', '', '', '', '', '', '']\n",
      "['gaypride', 'gEpRaid', '', '', '', '', '', '', '']\n",
      "['encor', 'âkOR', '', '', '', '', '', '', '']\n",
      "['uniq', 'ynik', '', '', '', '', '', '', '']\n",
      "['sciamma', 'sjama', '', '', '', '', '', '', '']\n",
      "['san', 'san', '', '', '', '', '', '', '']\n",
      "['fémis', 'femis', '', '', '', '', '', '', '']\n",
      "['coluche', 'kolyS', '', '', '', '', '', '', '']\n",
      "['gaëlle', 'gaEl', '', '', '', '', '', '', '']\n",
      "['léandre', 'leâdR', '', '', '', '', '', '', '']\n",
      "['next', 'nEkst', '', '', '', '', '', '', '']\n",
      "['geoffrey', 'ZofRE', '', '', '', '', '', '', '']\n",
      "['2', 'd2', '', '', '', '', '', '', '']\n",
      "['master', 'mastER', '', '', '', '', '', '', '']\n",
      "['too', 'tu', '', '', '', '', '', '', '']\n",
      "['pd', 'pede', '', '', '', '', '', '', '']\n",
      "['6', 'si', '', '', '', '', '', '', '']\n",
      "['cuter', 'k9te', '', '', '', '', '', '', '']\n",
      "['4', 'katR', '', '', '', '', '', '', '']\n",
      "['mathilde', 'matild', '', '', '', '', '', '', '']\n",
      "['aromantique', 'aRomâtik', '', '', '', '', '', '', '']\n",
      "['poulby', 'pulbi', '', '', '', '', '', '', '']\n",
      "['latex', 'latEks', '', '', '', '', '', '', '']\n",
      "['joris', 'ZoRis', '', '', '', '', '', '', '']\n",
      "['hum', 'm', '', '', '', '', '', '', '']\n",
      "['goten', 'gotEn', '', '', '', '', '', '', '']\n",
      "['bodypositif', 'bodipozitif', '', '', '', '', '', '', '']\n",
      "['hui', 'Hi', '', '', '', '', '', '', '']\n",
      "['99', 'katR@vêdizn9f', '', '', '', '', '', '', '']\n",
      "['hmm', 'mm', '', '', '', '', '', '', '']\n",
      "['hm', 'm', '', '', '', '', '', '', '']\n",
      "['shirley', 'S9RlE', '', '', '', '', '', '', '']\n",
      "['podcast', 'pOdkast', '', '', '', '', '', '', '']\n",
      "['harcel', 'aRsEl', '', '', '', '', '', '', '']\n",
      "['10', 'dis', '', '', '', '', '', '', '']\n",
      "['beh', 'be', '', '', '', '', '', '', '']\n",
      "['delleck', 'delEk', '', '', '', '', '', '', '']\n",
      "['pauvrine', 'povRin', '', '', '', '', '', '', '']\n",
      "['pouv', 'puv', '', '', '', '', '', '', '']\n",
      "['netflix', 'nEtfliks', '', '', '', '', '', '', '']\n",
      "['transidentité', 'trâsidâtite', '', '', '', '', '', '', '']\n",
      "['conscientisé', 'kôsjâtize', '', '', '', '', '', '', '']\n",
      "['larousse', 'laRus', '', '', '', '', '', '', '']\n",
      "['éric', 'eRik', '', '', '', '', '', '', '']\n",
      "['str', 'stR', '', '', '', '', '', '', '']\n",
      "['soignon', 'swaJô', '', '', '', '', '', '', '']\n",
      "['sophie', 'sofi', '', '', '', '', '', '', '']\n",
      "['trans', 'tRâs', '', '', '', '', '', '', '']\n",
      "['malc', 'malk', '', '', '', '', '', '', '']\n",
      "['pre', 'pR@', '', '', '', '', '', '', '']\n",
      "['bin', 'bî', '', '', '', '', '', '', '']\n",
      "['pra', 'pRa', '', '', '', '', '', '', '']\n",
      "['aujourd', 'oZuRd', '', '', '', '', '', '', '']\n",
      "['rivèle', 'RivEl', '', '', '', '', '', '', '']\n",
      "['wouhou', 'wu.u', '', '', '', '', '', '', '']\n",
      "['notam', 'notam', '', '', '', '', '', '', '']\n",
      "['at', 'at', '', '', '', '', '', '', '']\n",
      "['boureille', 'buREj', '', '', '', '', '', '', '']\n",
      "['av', 'av', '', '', '', '', '', '', '']\n",
      "['bodypositive', 'bodipositiv', '', '', '', '', '', '', '']\n",
      "['transexualité', 'tRâseksHalite', '', '', '', '', '', '', '']\n",
      "['euhm', '@m', '', '', '', '', '', '', '']\n",
      "['236', 'd2sâtrâtsis', '', '', '', '', '', '', '']\n",
      "['parce', 'paRs', '', '', '', '', '', '', '']\n",
      "['asmr', 'aEsEmER', '', '', '', '', '', '', '']\n",
      "['education', 'edykeSOn', '', '', '', '', '', '', '']\n",
      "['perç', 'pERs', '', '', '', '', '', '', '']\n",
      "['shirt', 'S9rt', '', '', '', '', '', '', '']\n",
      "['rébou', 'Rebu', '', '', '', '', '', '', '']\n",
      "['youtubeur', 'jutyb9r', '', '', '', '', '', '', '']\n",
      "['pa', 'pa', '', '', '', '', '', '', '']\n",
      "['popopoh', 'popopo', '', '', '', '', '', '', '']\n",
      "['meufs', 'm9f', '', '', '', '', '', '', '']\n",
      "['zon', 'zOn', '', '', '', '', '', '', '']\n",
      "['vou', 'vu', '', '', '', '', '', '', '']\n",
      "['adblock', 'adblOk', '', '', '', '', '', '', '']\n",
      "['quant', 'kât', '', '', '', '', '', '', '']\n",
      "['surcouches', 'syRkuS', '', '', '', '', '', '', '']\n",
      "['2005', 'd2milsîk', '', '', '', '', '', '', '']\n",
      "['9', 'n9f', '', '', '', '', '', '', '']\n",
      "['you', 'ju', '', '', '', '', '', '', '']\n",
      "['principals', 'pRîsipal', '', '', '', '', '', '', '']\n",
      "['fami', 'fami', '', '', '', '', '', '', '']\n",
      "['vict', 'vikt', '', '', '', '', '', '', '']\n",
      "['shamer', 'SEime', '', '', '', '', '', '', '']\n",
      "['lgbtq', 'ElZebeteky', '', '', '', '', '', '', '']\n",
      "['keum', 'k9m', '', '', '', '', '', '', '']\n",
      "['mascu', 'masky', '', '', '', '', '', '', '']\n",
      "['france', 'fRâs', '', '', '', '', '', '', '']\n",
      "['meuf', 'm9f', '', '', '', '', '', '', '']\n",
      "['anticapitalisme', 'âtikapitalism', '', '', '', '', '', '', '']\n",
      "['nexus6', 'neksyssis', '', '', '', '', '', '', '']\n",
      "['orgasmer', 'ORgasme', '', '', '', '', '', '', '']\n",
      "['maj', 'maZ', '', '', '', '', '', '', '']\n",
      "['friends', 'fREndz', '', '', '', '', '', '', '']\n",
      "['paraone', 'paRawan', '', '', '', '', '', '', '']\n",
      "['wouhouhou', 'wu.u.u', '', '', '', '', '', '', '']\n",
      "['fo', 'fu', '', '', '', '', '', '', '']\n",
      "['relou', 'R@lu', '', '', '', '', '', '', '']\n",
      "['cis', 'sis', '', '', '', '', '', '', '']\n",
      "['no', 'no', '', '', '', '', '', '', '']\n",
      "['panromantiques', 'pâRomâtik', '', '', '', '', '', '', '']\n",
      "['é', 'e', '', '', '', '', '', '', '']\n",
      "['masterclass', 'mast9Rklas', '', '', '', '', '', '', '']\n",
      "['vfd', 'veEfde', '', '', '', '', '', '', '']\n",
      "['youtube', 'jutyb', '', '', '', '', '', '', '']\n",
      "['teki', 'teki', '', '', '', '', '', '', '']\n",
      "['1', 'ê', '', '', '', '', '', '', '']\n",
      "['audioguide', 'odiogid', '', '', '', '', '', '', '']\n",
      "['jusqu', 'Zysk', '', '', '', '', '', '', '']\n",
      "['time', 'taim', '', '', '', '', '', '', '']\n",
      "['uterus', 'yteRys', '', '', '', '', '', '', '']\n",
      "['the', 'De', '', '', '', '', '', '', '']\n",
      "['adrien', 'adRijê', '', '', '', '', '', '', '']\n",
      "['adriléandre', 'adRileâdR', '', '', '', '', '', '', '']\n",
      "['mic', 'mik', '', '', '', '', '', '', '']\n",
      "{'roh': ('roh', 'Ro', '', '', '', '', '', '', ''), 'all': ('all', 'ol', '', '', '', '', '', '', ''), 'hyper': ('hyper', 'ipER', '', '', '', '', '', '', ''), 'fiote': ('fiote', 'fjot', '', '', '', '', '', '', ''), 'léadrien': ('léadrien', 'leadRiê', '', '', '', '', '', '', ''), 'ét': ('ét', 'et', '', '', '', '', '', '', ''), 'damso': ('damso', 'damso', '', '', '', '', '', '', ''), 'marion': ('marion', 'marjô', '', '', '', '', '', '', ''), 'thanks': ('thanks', 'Tâks', '', '', '', '', '', '', ''), 'cuts': ('cuts', 'k9t', '', '', '', '', '', '', ''), 'ttc': ('ttc', 'tetese', '', '', '', '', '', '', ''), 'alt': ('alt', 'alt', '', '', '', '', '', '', ''), 'suç': ('suç', 'sys', '', '', '', '', '', '', ''), 'larrouy': ('larrouy', 'laRwi', '', '', '', '', '', '', ''), 'cut': ('cut', 'k9t', '', '', '', '', '', '', ''), 'quee': ('quee', 'kwi', '', '', '', '', '', '', ''), '21': ('21', 'vîteî', '', '', '', '', '', '', ''), 'royan': ('royan', 'Rwajâ', '', '', '', '', '', '', ''), '2019': ('2019', 'd2mildizn9f', '', '', '', '', '', '', ''), '2018': ('2018', 'd2mildizHit', '', '', '', '', '', '', ''), 'éro': ('éro', 'eRo', '', '', '', '', '', '', ''), 'har': ('har', 'aR', '', '', '', '', '', '', ''), 'friendly': ('friendly', 'fREndli', '', '', '', '', '', '', ''), 'woh': ('woh', 'wo', '', '', '', '', '', '', ''), 'boris': ('boris', 'boRis', '', '', '', '', '', '', ''), 'monica': ('monica', 'monika', '', '', '', '', '', '', ''), 'sociétale': ('sociétale', 'sosjetal', '', '', '', '', '', '', ''), 'montmoreau': ('montmoreau', 'mômoRo', '', '', '', '', '', '', ''), 'very': ('very', 'veRi', '', '', '', '', '', '', ''), 'sexualit': ('sexualit', 'sEksHalit', '', '', '', '', '', '', ''), 'bear': ('bear', 'bER', '', '', '', '', '', '', ''), 'geek': ('geek', 'gik', '', '', '', '', '', '', ''), 'pansexuel': ('pansexuel', 'pâseksHEl', '', '', '', '', '', '', ''), 'not': ('not', 'nOt', '', '', '', '', '', '', ''), 'elena': ('elena', 'elena', '', '', '', '', '', '', ''), 'ohlala': ('ohlala', 'olala', '', '', '', '', '', '', ''), 'day': ('day', 'dEj', '', '', '', '', '', '', ''), 'cringy': ('cringy', 'krinZi', '', '', '', '', '', '', ''), 'gray': ('gray', 'gRE', '', '', '', '', '', '', ''), 'trash': ('trash', 'tRaS', '', '', '', '', '', '', ''), 'qu': ('qu', 'k', '', '', '', '', '', '', ''), 'ç': ('ç', 's', '', '', '', '', '', '', ''), 'james': ('james', 'ZEms', '', '', '', '', '', '', ''), '50': ('50', 'sîkât', '', '', '', '', '', '', ''), 'garcia': ('garcia', 'gaRsia', '', '', '', '', '', '', ''), 'louis': ('louis', 'lwi', '', '', '', '', '', '', ''), 'chev': ('chev', 'S@v', '', '', '', '', '', '', ''), 'el': ('el', 'El', '', '', '', '', '', '', ''), 'ouh': ('ouh', 'u', '', '', '', '', '', '', ''), 'instagram': ('instagram', 'êstagRam', '', '', '', '', '', '', ''), 'fair': ('fair', 'fEr', '', '', '', '', '', '', ''), 'ouhouhouh': ('ouhouhouh', 'u.u.u', '', '', '', '', '', '', ''), 'queer': ('queer', 'kwir', '', '', '', '', '', '', ''), 'sex': ('sex', 'sEks', '', '', '', '', '', '', ''), 'auteure': ('auteure', 'ot9r', '', '', '', '', '', '', ''), 'arg': ('arg', 'aRg', '', '', '', '', '', '', ''), 'mélo': ('mélo', 'melo', '', '', '', '', '', '', ''), 'podcasts': ('podcasts', 'pOdkast', '', '', '', '', '', '', ''), 'yes': ('yes', 'jEs', '', '', '', '', '', '', ''), 'gé': ('gé', 'Ze', '', '', '', '', '', '', ''), 'hypersexuels': ('hypersexuels', 'ipERseksHEl', '', '', '', '', '', '', ''), 'butch': ('butch', 'b9tS', '', '', '', '', '', '', ''), 'grossophobie': ('grossophobie', 'gRosofobi', '', '', '', '', '', '', ''), 'laserquest': ('laserquest', 'lazERkwEst', '', '', '', '', '', '', ''), 'rer': ('rer', 'ER2ER', '', '', '', '', '', '', ''), 'quelqu': ('quelqu', 'kElk', '', '', '', '', '', '', ''), 'raclement': ('raclement', 'Rakl@mâ', '', '', '', '', '', '', ''), 're': ('re', 'R@', '', '', '', '', '', '', ''), '3': ('3', 'tRwa', '', '', '', '', '', '', ''), 'sml': ('sml', 'EsEMEl', '', '', '', '', '', '', ''), 'diaz': ('diaz', 'diaz', '', '', '', '', '', '', ''), 'melo': ('melo', 'melo', '', '', '', '', '', '', ''), '7': ('7', 'sEt', '', '', '', '', '', '', ''), 'import': ('import', 'îpoRt', '', '', '', '', '', '', ''), 'ordr': ('ordr', 'oRdR', '', '', '', '', '', '', ''), 'couilles': ('couilles', 'kuj', '', '', '', '', '', '', ''), 'sexuali': ('sexuali', 'seksHali', '', '', '', '', '', '', ''), 'pun': ('pun', 'p9n', '', '', '', '', '', '', ''), 'christophe': ('christophe', 'kristof', '', '', '', '', '', '', ''), 'men': ('men', 'mEn', '', '', '', '', '', '', ''), 'hypersexuelle': ('hypersexuelle', 'ipERseksHEl', '', '', '', '', '', '', ''), 'meh': ('meh', 'mE', '', '', '', '', '', '', ''), 'crédo': ('crédo', 'kRedo', '', '', '', '', '', '', ''), 'bi': ('bi', 'bi', '', '', '', '', '', '', ''), 'lgbt': ('lgbt', 'ElZebete', '', '', '', '', '', '', ''), 'disneyland': ('disneyland', 'disnelâd', '', '', '', '', '', '', ''), 'céline': ('céline', 'selin', '', '', '', '', '', '', ''), 'thank': ('thank', 'fEnk', '', '', '', '', '', '', ''), 'hétéro': ('hétéro', 'eteRo', '', '', '', '', '', '', ''), 'oro': ('oro', 'oRo', '', '', '', '', '', '', ''), 'éta': ('éta', 'eta', '', '', '', '', '', '', ''), 'émifion': ('émifion', 'emifjô', '', '', '', '', '', '', ''), 'sexu': ('sexu', 'seksy', '', '', '', '', '', '', ''), 'jou': ('jou', 'Zu', '', '', '', '', '', '', ''), 'co': ('co', 'ko', '', '', '', '', '', '', ''), 'hypersexuel': ('hypersexuel', 'ipERseksyEl', '', '', '', '', '', '', ''), 'pénétratif': ('pénétratif', 'penetRatif', '', '', '', '', '', '', ''), 'ca': ('ca', 'ka', '', '', '', '', '', '', ''), 'rachel': ('rachel', 'RaSEl', '', '', '', '', '', '', ''), 'thierry': ('thierry', 'tieRi', '', '', '', '', '', '', ''), 'julia': ('julia', 'Zylia', '', '', '', '', '', '', ''), 'one': ('one', 'wan', '', '', '', '', '', '', ''), 'miranda': ('miranda', 'miRâda', '', '', '', '', '', '', ''), 'scèn': ('scèn', 'sEn', '', '', '', '', '', '', ''), 'normer': ('normer', 'nORme', '', '', '', '', '', '', ''), 'anti': ('anti', 'âti', '', '', '', '', '', '', ''), 'gaypride': ('gaypride', 'gEpRaid', '', '', '', '', '', '', ''), 'encor': ('encor', 'âkOR', '', '', '', '', '', '', ''), 'uniq': ('uniq', 'ynik', '', '', '', '', '', '', ''), 'sciamma': ('sciamma', 'sjama', '', '', '', '', '', '', ''), 'san': ('san', 'san', '', '', '', '', '', '', ''), 'fémis': ('fémis', 'femis', '', '', '', '', '', '', ''), 'coluche': ('coluche', 'kolyS', '', '', '', '', '', '', ''), 'gaëlle': ('gaëlle', 'gaEl', '', '', '', '', '', '', ''), 'léandre': ('léandre', 'leâdR', '', '', '', '', '', '', ''), 'next': ('next', 'nEkst', '', '', '', '', '', '', ''), 'geoffrey': ('geoffrey', 'ZofRE', '', '', '', '', '', '', ''), '2': ('2', 'd2', '', '', '', '', '', '', ''), 'master': ('master', 'mastER', '', '', '', '', '', '', ''), 'too': ('too', 'tu', '', '', '', '', '', '', ''), 'pd': ('pd', 'pede', '', '', '', '', '', '', ''), '6': ('6', 'si', '', '', '', '', '', '', ''), 'cuter': ('cuter', 'k9te', '', '', '', '', '', '', ''), '4': ('4', 'katR', '', '', '', '', '', '', ''), 'mathilde': ('mathilde', 'matild', '', '', '', '', '', '', ''), 'aromantique': ('aromantique', 'aRomâtik', '', '', '', '', '', '', ''), 'poulby': ('poulby', 'pulbi', '', '', '', '', '', '', ''), 'latex': ('latex', 'latEks', '', '', '', '', '', '', ''), 'joris': ('joris', 'ZoRis', '', '', '', '', '', '', ''), 'hum': ('hum', 'm', '', '', '', '', '', '', ''), 'goten': ('goten', 'gotEn', '', '', '', '', '', '', ''), 'bodypositif': ('bodypositif', 'bodipozitif', '', '', '', '', '', '', ''), 'hui': ('hui', 'Hi', '', '', '', '', '', '', ''), '99': ('99', 'katR@vêdizn9f', '', '', '', '', '', '', ''), 'hmm': ('hmm', 'mm', '', '', '', '', '', '', ''), 'hm': ('hm', 'm', '', '', '', '', '', '', ''), 'shirley': ('shirley', 'S9RlE', '', '', '', '', '', '', ''), 'podcast': ('podcast', 'pOdkast', '', '', '', '', '', '', ''), 'harcel': ('harcel', 'aRsEl', '', '', '', '', '', '', ''), '10': ('10', 'dis', '', '', '', '', '', '', ''), 'beh': ('beh', 'be', '', '', '', '', '', '', ''), 'delleck': ('delleck', 'delEk', '', '', '', '', '', '', ''), 'pauvrine': ('pauvrine', 'povRin', '', '', '', '', '', '', ''), 'pouv': ('pouv', 'puv', '', '', '', '', '', '', ''), 'netflix': ('netflix', 'nEtfliks', '', '', '', '', '', '', ''), 'transidentité': ('transidentité', 'trâsidâtite', '', '', '', '', '', '', ''), 'conscientisé': ('conscientisé', 'kôsjâtize', '', '', '', '', '', '', ''), 'larousse': ('larousse', 'laRus', '', '', '', '', '', '', ''), 'éric': ('éric', 'eRik', '', '', '', '', '', '', ''), 'str': ('str', 'stR', '', '', '', '', '', '', ''), 'soignon': ('soignon', 'swaJô', '', '', '', '', '', '', ''), 'sophie': ('sophie', 'sofi', '', '', '', '', '', '', ''), 'trans': ('trans', 'tRâs', '', '', '', '', '', '', ''), 'malc': ('malc', 'malk', '', '', '', '', '', '', ''), 'pre': ('pre', 'pR@', '', '', '', '', '', '', ''), 'bin': ('bin', 'bî', '', '', '', '', '', '', ''), 'pra': ('pra', 'pRa', '', '', '', '', '', '', ''), 'aujourd': ('aujourd', 'oZuRd', '', '', '', '', '', '', ''), 'rivèle': ('rivèle', 'RivEl', '', '', '', '', '', '', ''), 'wouhou': ('wouhou', 'wu.u', '', '', '', '', '', '', ''), 'notam': ('notam', 'notam', '', '', '', '', '', '', ''), 'at': ('at', 'at', '', '', '', '', '', '', ''), 'boureille': ('boureille', 'buREj', '', '', '', '', '', '', ''), 'av': ('av', 'av', '', '', '', '', '', '', ''), 'bodypositive': ('bodypositive', 'bodipositiv', '', '', '', '', '', '', ''), 'transexualité': ('transexualité', 'tRâseksHalite', '', '', '', '', '', '', ''), 'euhm': ('euhm', '@m', '', '', '', '', '', '', ''), '236': ('236', 'd2sâtrâtsis', '', '', '', '', '', '', ''), 'parce': ('parce', 'paRs', '', '', '', '', '', '', ''), 'asmr': ('asmr', 'aEsEmER', '', '', '', '', '', '', ''), 'education': ('education', 'edykeSOn', '', '', '', '', '', '', ''), 'perç': ('perç', 'pERs', '', '', '', '', '', '', ''), 'shirt': ('shirt', 'S9rt', '', '', '', '', '', '', ''), 'rébou': ('rébou', 'Rebu', '', '', '', '', '', '', ''), 'youtubeur': ('youtubeur', 'jutyb9r', '', '', '', '', '', '', ''), 'pa': ('pa', 'pa', '', '', '', '', '', '', ''), 'popopoh': ('popopoh', 'popopo', '', '', '', '', '', '', ''), 'meufs': ('meufs', 'm9f', '', '', '', '', '', '', ''), 'zon': ('zon', 'zOn', '', '', '', '', '', '', ''), 'vou': ('vou', 'vu', '', '', '', '', '', '', ''), 'adblock': ('adblock', 'adblOk', '', '', '', '', '', '', ''), 'quant': ('quant', 'kât', '', '', '', '', '', '', ''), 'surcouches': ('surcouches', 'syRkuS', '', '', '', '', '', '', ''), '2005': ('2005', 'd2milsîk', '', '', '', '', '', '', ''), '9': ('9', 'n9f', '', '', '', '', '', '', ''), 'you': ('you', 'ju', '', '', '', '', '', '', ''), 'principals': ('principals', 'pRîsipal', '', '', '', '', '', '', ''), 'fami': ('fami', 'fami', '', '', '', '', '', '', ''), 'vict': ('vict', 'vikt', '', '', '', '', '', '', ''), 'shamer': ('shamer', 'SEime', '', '', '', '', '', '', ''), 'lgbtq': ('lgbtq', 'ElZebeteky', '', '', '', '', '', '', ''), 'keum': ('keum', 'k9m', '', '', '', '', '', '', ''), 'mascu': ('mascu', 'masky', '', '', '', '', '', '', ''), 'france': ('france', 'fRâs', '', '', '', '', '', '', ''), 'meuf': ('meuf', 'm9f', '', '', '', '', '', '', ''), 'anticapitalisme': ('anticapitalisme', 'âtikapitalism', '', '', '', '', '', '', ''), 'nexus6': ('nexus6', 'neksyssis', '', '', '', '', '', '', ''), 'orgasmer': ('orgasmer', 'ORgasme', '', '', '', '', '', '', ''), 'maj': ('maj', 'maZ', '', '', '', '', '', '', ''), 'friends': ('friends', 'fREndz', '', '', '', '', '', '', ''), 'paraone': ('paraone', 'paRawan', '', '', '', '', '', '', ''), 'wouhouhou': ('wouhouhou', 'wu.u.u', '', '', '', '', '', '', ''), 'fo': ('fo', 'fu', '', '', '', '', '', '', ''), 'relou': ('relou', 'R@lu', '', '', '', '', '', '', ''), 'cis': ('cis', 'sis', '', '', '', '', '', '', ''), 'no': ('no', 'no', '', '', '', '', '', '', ''), 'panromantiques': ('panromantiques', 'pâRomâtik', '', '', '', '', '', '', ''), 'é': ('é', 'e', '', '', '', '', '', '', ''), 'masterclass': ('masterclass', 'mast9Rklas', '', '', '', '', '', '', ''), 'vfd': ('vfd', 'veEfde', '', '', '', '', '', '', ''), 'youtube': ('youtube', 'jutyb', '', '', '', '', '', '', ''), 'teki': ('teki', 'teki', '', '', '', '', '', '', ''), '1': ('1', 'ê', '', '', '', '', '', '', ''), 'audioguide': ('audioguide', 'odiogid', '', '', '', '', '', '', ''), 'jusqu': ('jusqu', 'Zysk', '', '', '', '', '', '', ''), 'time': ('time', 'taim', '', '', '', '', '', '', ''), 'uterus': ('uterus', 'yteRys', '', '', '', '', '', '', ''), 'the': ('the', 'De', '', '', '', '', '', '', ''), 'adrien': ('adrien', 'adRijê', '', '', '', '', '', '', ''), 'adriléandre': ('adriléandre', 'adRileâdR', '', '', '', '', '', '', ''), 'mic': ('mic', 'mik', '', '', '', '', '', '', '')}\n",
      "groupeEAF Rush Ep2_Fanny.eaf\n",
      "groupeEAF Rush Ep2_Leandre.eaf\n",
      "groupeEAF Temoin.eaf\n"
     ]
    }
   ],
   "source": [
    "debug=0\n",
    "for repGroupe in repGroupes[:]:\n",
    "    print ()\n",
    "    print (repGroupe)\n",
    "    groupeInconnus=lireInconnus(repGroupe)\n",
    "    groupeEAFs=listerEAFs(repGroupe)\n",
    "    for groupeEAF in groupeEAFs:\n",
    "        print (\"groupeEAF\",groupeEAF.split(\"/\")[-1])\n",
    "        subBdlexique,groupeInconnus=faireLexique(groupeEAF,groupeInconnus)\n",
    "        newEAF=transcrireEAF(groupeEAF,subBdlexique)\n",
    "        fNewEAF=groupeEAF.replace(\".eaf\",\"-phonetique.eaf\")\n",
    "        newEAF.write(fNewEAF, pretty_print=True, encoding='utf-8', xml_declaration=True)\n",
    "        newTRS=transcrireTRS(groupeEAF,subBdlexique)\n",
    "        nomXML=groupeEAF.replace(\".eaf\",\".xml\")\n",
    "        if nomXML!=groupeEAF:\n",
    "            newTRS.write(nomXML, pretty_print=True, encoding='utf-8', xml_declaration=True)\n",
    "        else:\n",
    "            print (\"pb de nom EAF\",nomEAF)\n",
    "    ecrireInconnus(groupeInconnus,repGroupe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"p(eu)t-êt(re)\" => \"p't-êt'\" pøtEtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
