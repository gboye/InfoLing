{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement des EAF avec BDLexique\n",
    "- la phonétisation dans un fichier-phonetique.eaf\n",
    "- les éléments de BDLexique et les balises complémentaires dans un fichier.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys, codecs, re, glob\n",
    "import pdb # ajouter pdb.set_trace() à l'endroit où on veut le débugueur\n",
    "from lxml import etree as ET\n",
    "import bs4\n",
    "# import xml.etree.cElementTree as ET\n",
    "import os, fnmatch, unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparatifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annee=22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créer un parser XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ET.XMLParser(remove_blank_text=True)\n",
    "debug=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lire BDLexique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fichierLexique=\"/Users/gilles/ownCloud/Cours/Bordeaux/L2-XML/XML-Ressources/bdlexique.txt\"\n",
    "fichier_exceptions=True\n",
    "\n",
    "lexicon=codecs.open(fichierLexique,\"r\",encoding='utf8')\n",
    "lBdlexique=lexicon.readlines()\n",
    "lexicon.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdlexique={}\n",
    "for line in lBdlexique:\n",
    "    line=line.strip()\n",
    "    p=line.split(u';')\n",
    "    if p[2]==\"@\" and not p[3] in [\"N\",\"V\",\"J\",\"K\"]:\n",
    "        p[1]+=p[2]\n",
    "        p[2]=\"\"\n",
    "        if len(p)<7:\n",
    "            for i in range(len(p)+1,7):\n",
    "                p.append(\"\")\n",
    "    bdlexique[p[0].lower()]=(p[0],p[1],p[2],p[3],p[4],p[5],p[6],p[7],p[8],p[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "connecteurs=[\n",
    "    u\"et\", u\"alors\", u\"du coup\", u\"sinon\", u\"par contre\", u\"ça veut dire\", u\"enfin\",\n",
    "u\"après\", u\"donc\", u\"puisque\", u\"puisqu'\", u\"en fait\", u\"mais\", u\"parce que\", u\"parce qu'\", u\"même si\" , u\"d'abord\", u\"et puis\"\n",
    "]\n",
    "motTheme=[u\"cible\",u\"frontières\",u\"accueillir\",u\"démonstrations\",\n",
    "          u\"accompagner\",u\"contre-courant\",u\"importantes\",u\"droit de bouger\",\n",
    "          u\"peur\",u\"solution\",u\"durable\",u\"refuge\",u\"assassins\",u\"tueurs\",u\"violeurs\",\n",
    "          u\"fuient\",u\"misère\",u\"chaos\",u\"circulation\",u\"liberté\",u\"installation\",\n",
    "          u\"solidarité\",u\"migrants\",u\"ouverture\",u\"frontières\",u\"conditions\",u\"réflexe\",\n",
    "          u\"normal\",u\"sain\",u\"accueillir\",u\"moyens\",u\"intégrés\",u\"flux\",u\"migratoires\",\n",
    "          u\"énormes\",u\"moyens\",u\"structures\",u\"loger\",u\"répondre\",u\"peuples\",u\"coopération\",\n",
    "          u\"indéniable\",u\"réfugiés\",u\"digne\",u\"prendre la main\",u\"équilibrée\",u\"accueil\",\n",
    "          u\"correctement\",u\"favorable\",u\"hébergés\",u\"mise à l’abri\",u\"centre humanitaire\",\n",
    "          u\"situation\",u\"convenable\",u\"heurtée\",u\"valeurs\",u\"république\",u\"comportement\",\n",
    "          u\"politique\",u\"associations\",u\"hébergement\",u\"respect\",u\"lien\",u\"confiance\",\n",
    "          u\"travailleur\",u\"social\",u\"policiers\",u\"gazent\",u\"détruisent\",u\"nourriture\",\n",
    "          u\"refusent\",u\"points d’eau\",u\"négation\",u\"partent\",u\"parcours\",u\"migration\",\n",
    "          u\"terribles\",u\"touche\",u\"cimetière\",u\"choquant\",u\"scandaleux\",u\"dignité\",\n",
    "          u\"impression\",u\"responsabilités\",u\"multiplication\",u\"conflits\",u\"excessivement\",\n",
    "          u\"violents\",u\"solution\",u\"seule\",u\"quitter\",u\"victimes\",u\"esclavage\",\n",
    "          u\"regression\",u\"humanité\",u\"guerre\",u\"état\",u\"pauvreté\",u\"pays\",u\"conditions\",\n",
    "          u\"respect\",u\"organisme\",u\"ficher\",u\"contrôler\",u\"demande\",u\"papiers\",u\"droits\",\n",
    "          u\"échec\",u\"aide\",u\"développement\",u\"vivre\",u\"famille\",u\"terrain\",u\"voir\",\n",
    "          u\"souffrance\",u\"regression\",u\"France\",u\"richesse\",u\"humanité\",u\"extraordinaire\",\n",
    "          u\"intérger\",u\"asile\",u\"question\",u\"temporaire\",u\"débat\",u\"souhaitons\",\n",
    "          u\"propositions\",u\"européen\",u\"rapprocher\",u\"proches\",u\"position\",u\"absurdité\",\n",
    "          u\"ONG\",u\"aider\",u\"détresse\",u\"lois\",u\"règles\",u\"principes\",u\"humanité\",\n",
    "          u\"vulnérable\",u\"enfants\",u\"sauver\",u\"vies\",u\"générosité\",u\"classes\",\n",
    "          u\"francophones\",u\"découverte\",u\"langue\",u\"logements\",u\"associations\",\n",
    "          u\"continuerai\",u\"nouveaux\",u\"clé\",u\"chômage\",u\"héberger\",u\"employer\",u\"former\",\n",
    "          u\"éduquer\",u\"milliers\",u\"vaincre\",u\"réticences\",u\"acccepter\",u\"aménage\",\n",
    "          u\"refuges\",u\"régions\",u\"viennent\",u\"zones\",u\"protégées\",u\"normales\",u\"chez-eux\",\n",
    "          u\"circuler\",u\"librement\",u\"Allemagne\",u\"obligations\",u\"humanitaire\",u\"préserver\",\n",
    "          u\"populations\",u\"bataille\",u\"hégémonique\",u\"bataille\",u\"s'installer\",u\"mois\",\n",
    "          u\"désaccord\",u\"moins\",u\"accueillant\",u\"bataille\",u\"idéologique\",u\"pression\",\n",
    "          u\"chercher\",u\"immigrés\",u\"de force\",u\"délogés\",u\"expulsés\",u\"xénophobie\",\n",
    "          u\"racistes\",u\"recul\",u\"idées\",u\"argent\",u\"dramatique\",u\"problèmes\",u\"économiques\",\n",
    "          u\"interventions\",u\"militaires\",u\"famine\",u\"massacres\",u\"répression\",u\"bombardé\",\n",
    "          u\"manqué\",u\"devoir\",u\"indigne\",u\"déposer\",u\"demande d'asile\",u\"absurde\",\n",
    "          u\"campements\",u\"regrette\",u\"dispositifs\",u\"pérennes\",u\"divergence\",\n",
    "          u\"appréciation\",u\"reconstitution\",u\"campements\",u\"crise\",u\"emballement\",\n",
    "          u\"otage\",u\"noyer\",u\"guerre\",u\"bombardement\",u\"morts\",u\"difficultés\",u\"blocage\",\n",
    "          u\"maltraiter\",u\"seul\",u\"privation\",u\"absurde\",u\"illusion\",u\"mensonge\",u\"ghettos\",\n",
    "          u\"français\",u\"divisés\",u\"meurent\",u\"fuient\",u\"devoirs\",u\"moraux\",\n",
    "          u\"centres de rétention\",u\"protection\",u\"cause\",u\"guerre\",u\"lieu\",u\"couteux\",\n",
    "          u\"charge\",u\"services\",u\"gratuits\",u\"système\",u\"gestion\",u\"patrimoine\",\n",
    "          u\"nationale\",u\"importante\",u\"fossé\",u\"travail\",u\"difficulté\",u\"maîtrise\",\n",
    "          u\"suspendre\",u\"shengen\",u\"arrêter\",u\"immigration\",u\"communautarisme\",\n",
    "          u\"fondamentalisme\",u\"islamiste\",u\"gel\",u\"constructions\",u\"mosquées\",u\"islam\",\n",
    "          u\"radicaux\",u\"vivier\",u\"radicalisation\",u\"soldats\",u\"djihadisme\",u\"terroristes\",\n",
    "          u\"puissance\",u\"terreau\",u\"frappent\",u\"clandestine\",u\"massive\",u\"insécurité\",\n",
    "          u\"terrifiante\",u\"forces de l'ordre\",u\"mesures législatives\",u\"suppression\",\n",
    "          u\"enrayer\",u\"processus\",u\"récupérer\",u\"clandestins\",u\"absence\",u\"bateaux\",\n",
    "          u\"en mer\",u\"ramener\",u\"facilite\",u\"réseaux\",u\"criminels\",u\"laisser\",u\"mourir\",\n",
    "          u\"sauver\",u\"ramener\",u\"port d’origine\",u\"corrompus\",u\"passeurs\",u\"arme\",\n",
    "          u\"catastrophique\",u\"occupation\",u\"illégale\",u\"inexpulsable\",u\"engrenage\",\n",
    "          u\"procédures\",u\"demandeurs\",u\"déboutés\",u\"économique\",u\"raisons\",u\"traverser\",\n",
    "          u\"sortis\",u\"territoire\",u\"arrivés\",u\"origine\",u\"critères\",u\"extrêmement\",\n",
    "          u\"incitatif\",u\"centres\",u\"AME\",u\"CMU\",u\"ensemble\",u\"folle\",u\"coût\",u\"faramineux\",\n",
    "          u\"malvenu\",u\"opposition\",u\"opposée\",u\"communautarisme\",u\"fondamentalisme\",\n",
    "          u\"imposer\",u\"ennemi\",u\"contre\",u\"rejet\",u\"anti-démocratique\",u\"enfermemement\",\n",
    "          u\"séparation\",u\"pourissement\",u\"étranger\",u\"drame\",u\"faillite\",u\"déloyale\",\n",
    "          u\"ménage\",\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatage divers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time2TRS(time):\n",
    "    return unicode(float(time)/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gestion API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "consonnes=['k\"', '(kt)\"', 'n\"', 'p\"', 'R\"', '@t\"', 't\"', '-V', '+V', '@z\"', 'z\"']\n",
    "voyelles=[\"H\", \"j\", \"w\", \"E\", \"a\", \"2\", \"9\", \"6\", \"@\", \"y\", \"u\", \"O\", u\"ò\", \"o\", \"e\", u\"è\", u\"ê\", u\"û\", u\"ô\", \"i\"]\n",
    "\n",
    "# traduire SAMPA-BDLex en API\n",
    "\n",
    "def sampa2api(sampa):\n",
    "    if isinstance(sampa,str):\n",
    "        api=sampa.decode(\"utf8\")\n",
    "    else:\n",
    "        api=sampa\n",
    "    api=api.replace(u'n\"',u'n') \n",
    "    api=api.replace(u't\"',u't') \n",
    "    api=api.replace(u'z\"',u'z') \n",
    "    api=api.replace(u'R\"',u'ʁ') \n",
    "    api=api.replace(u'p\"',u'p') \n",
    "    api=api.replace(u'S',u'ʃ') \n",
    "    api=api.replace(u'Z',u'ʒ')\n",
    "    api=api.replace(u'N',u'ŋ')\n",
    "    api=api.replace(u'J',u'ɲ')\n",
    "    api=api.replace(u'r',u'ʁ') \n",
    "    api=api.replace(u'H',u'ɥ')\n",
    "    api=api.replace(u'E',u'ɛ')\n",
    "    api=api.replace(u'2',u'ø')\n",
    "    api=api.replace(u'9',u'œ')\n",
    "    api=api.replace(u'6',u'ə')\n",
    "    api=api.replace(u'O',u'ɔ')\n",
    "    api=api.replace(u'è',u'e')   \n",
    "    api=api.replace(u'ò',u'o')    \n",
    "    api=api.replace(u'â',u'ɑ̃')   \n",
    "    api=api.replace(u'ê',u'ɛ̃')   \n",
    "    api=api.replace(u'û',u'œ̃')  \n",
    "    api=api.replace(u'ô',u'ɔ̃')       \n",
    "    api=api.replace(u'@',u'ə')\n",
    "#     api=api.replace(u'R',u'ʁ') \n",
    "    return api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gestion Liaison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liaison possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "algorithme\n",
    "\n",
    "+ si le mot courant et le suivant ne sont pas dans lexicon, pas de liaison\n",
    "+ si le mot a une consonne dans le champ de la voyelle de liaison, check1 est vrai\n",
    "+ si le mot suivant commence par une voyelle, check2 est vrai\n",
    "\n",
    "  si check1 et check2 sont vrais, il y a liaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formerLiaison(mot1,mot2,contexteLiaison=False):\n",
    "    if mot1[1][1]:\n",
    "        result=sampa2api(mot1[1][1])\n",
    "        if contexteLiaison:\n",
    "#             print mot1, \n",
    "#             print mot2\n",
    "            if mot1[1][2] in consonnes and mot2[1][1][0] in voyelles:\n",
    "                result+=sampa2api(mot1[1][2].strip(\"()\"))\n",
    "    else:\n",
    "        result=\"***%s***\"%mot1[1][0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tɛkst\n"
     ]
    }
   ],
   "source": [
    "print formerLiaison([\"text\",(\"1\",\"tEkst\",\"3\")],None,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liaison obligatoire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "algorithme\n",
    "\n",
    "+ si le mot courant et le suivant sont dans un des cas de figure, il y a liaison\n",
    "+ sinon pas de liaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def liaison_obligatoire(mot1,mot2):\n",
    "    determinant=[\"d\", \"P\"]\n",
    "    nom=[\"N\", \"G\", \"M\"]\n",
    "    adjectif=[\"J\", \"G\", \"M\"]\n",
    "    pronompers=[\"P\"]\n",
    "    verbe=[\"V\"]\n",
    "    cat1=mot1[1][3]\n",
    "    cat2=mot2[1][3]\n",
    "#     print cat1,cat2\n",
    "\n",
    "    if cat1 in determinant and cat2 in nom :\n",
    "        return True\n",
    "\n",
    "    elif cat1 in determinant and cat2 in adjectif :\n",
    "        return True\n",
    "\n",
    "    elif cat1 in adjectif and cat2 in nom :\n",
    "        return True\n",
    "    \n",
    "    elif cat1 in pronompers and cat2 in verbe :\n",
    "        return True\n",
    "\n",
    "    elif cat1 in verbe and cat2 in pronompers :\n",
    "        return True\n",
    "\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cas de figure possibles:\n",
    "\n",
    "- DET + N\n",
    "    * ri + N:   d'animal, \n",
    "    * di + N:   certains éléphants\n",
    "    * rd + N:   les animaux\n",
    "    * dd + N:   ces étés, cet été\n",
    "    * dp + N:   ton anorak\n",
    "    * rc + N:   aux armes\n",
    "- DET + ADJ:\n",
    "    * ri + ADJ:   d'énormes\n",
    "    * di + ADJ:   plusieurs immenses\n",
    "    * rd + ADJ:   les immenses\n",
    "    * dd + ADJ:   cet immense\n",
    "    * dp + ADJ:   son immense\n",
    "    * rc + ADJ:   aux immenses\n",
    "- PERS + V:\n",
    "    * SS + V:   m'épate\n",
    "- V + PRO PERS: \n",
    "    * V + SS:   vont-ils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "algorithme\n",
    "\n",
    "+ si le mot courant et le suivant sont dans un des cas de figure, il y a liaison\n",
    "+ sinon pas de liaison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liaison facultative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vérifier si la liaison est facultative\n",
    "def liaison_facultative(mot1,mot2):\n",
    "    #pdb.set_trace()\n",
    "    nom=[\"N\", \"G\", \"M\"]\n",
    "    pluriel=[\"MP\", \"FP\"]\n",
    "    adjectif=[\"J\", \"G\", \"M\"]\n",
    "    verbe=[\"V\"]\n",
    "    pronompers=[\"P\"]\n",
    "    adverbe=[\"A\"]\n",
    "    preposition=[\"p\"]\n",
    "    cat1=mot1[1][3]\n",
    "    cat2=mot2[1][3]\n",
    "    \n",
    "    if (cat1 in nom) and (mot1[1][4] in pluriel) and (cat2 in adjectif) : \n",
    "        return True\n",
    "\n",
    "    elif (cat1 in verbe) and (cat2 not in pronompers):\n",
    "        return True\n",
    "\n",
    "    elif cat1 in adverbe :\n",
    "        return True\n",
    "    \n",
    "    elif cat1 in preposition : \n",
    "        return True\n",
    "\n",
    "    else :\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cas de figure possibles :\n",
    "\n",
    "- N pl + ADJ: \n",
    "    * N + ADJ: monstres énormes \n",
    "    * G + ADJ: rivaux énormes\n",
    "- VERBE + TOUT-SAUF-PRO-PERS:\n",
    "    * V + N sont éléphants\n",
    "    * V + G sommes abdicaires\n",
    "    * V + V sommes assis\n",
    "    * V + A sommes admirablement\n",
    "    * V + p sommes autour de\n",
    "    * V + di ont aucune\n",
    "    * V + rc sommes au\n",
    "- ADV + QQCH:\n",
    "    * ADV + N vraiment abruti\n",
    "    * ADV + G vraiment abandonné\n",
    "    * ADV + V vraiment aimé\n",
    "    * ADV + J vraiment étonnant\n",
    "    * ADV + ss vraiment ils\n",
    "    * ADV + A vraiment étonnamment\n",
    "    * ADV + p vraiment attendu\n",
    "    * ADV + di vraiment autre \n",
    "    * ADV + rc vraiment au\n",
    "- PREP + QQCH:\n",
    "    * PREP + N très amoureux\n",
    "    * PREP + G très abandonné\n",
    "    * PREP + V très aimé\n",
    "    * PREP + J très étonnant\n",
    "    * PREP + SS très ils\n",
    "    * PREP + A très étonnamment\n",
    "    * PREP + p très attendu\n",
    "    * PREP + di très autre\n",
    "    * PREP + rc très au\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Partie 1\n",
    "*chaque phrase est prise individuellement,\n",
    "    * découpée en blocs,\n",
    "        * qui sont chacuns trimés si ce sont des mots\n",
    "        * s'il y a plusieurs mots dans le bloc, ils sont séparés\n",
    "    + Partie 2\n",
    "    * pour chaque couple de mots\n",
    "        * si la liaison est possible,\n",
    "            * et qu'elle est obligatoire, l'api avec la liaison est généré\n",
    "            * et qu'elle est facultative,\n",
    "                * si l'utilisateur l'a choisi, l'api avec la liaison est généré\n",
    "                * sinon l'api sans la liaison est généré\n",
    "\n",
    "        + Partie 3\n",
    "        * si la liaison n'est pas possible,\n",
    "            * si le mot est dans bdlex, l'api est généré\n",
    "            * sinon le mot est laissé tel quel (il a déjà les étoiles)        \n",
    "\n",
    "    * pour le dernier mot de la phrase, \n",
    "        * si le mot est dans bdlex, l'api est généré\n",
    "        * sinon le mot est laissé tel quel (il a déjà les étoiles) \n",
    "\n",
    "+ Partie 4\n",
    "* le message à l'utilisateur et la phrase en api est imprimée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparer structures Tours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "altEspace=ur\"\\s|_\"\n",
    "connecteurs=[\n",
    "    u\"et\", u\"alors\", u\"du coup\", u\"sinon\", u\"par contre\", u\"ça veut dire\", u\"enfin\",\n",
    "u\"après\", u\"donc\", u\"puisque\", u\"puisqu'\", u\"en fait\", u\"mais\", u\"parce que\", u\"parce qu'\", u\"même si\" , u\"d'abord\", u\"et puis\"\n",
    "]\n",
    "immigration=[u\"cible\",u\"frontières\",u\"accueillir\",u\"démonstrations\",u\"accompagner\",u\"contre-courant\",u\"importantes\",u\"droit de bouger\",u\"peur\",u\"solution\",u\"durable\",u\"refuge\",u\"assassins\",u\"tueurs\",u\"violeurs\",u\"fuient\",u\"misère\",u\"chaos\",u\"circulation\",u\"liberté\",u\"installation\",u\"solidarité\",u\"migrants\",u\"ouverture\",u\"frontières\",u\"conditions\",u\"réflexe\",u\"normal\",u\"sain\",u\"accueillir\",u\"moyens\",u\"intégrés\",u\"flux\",u\"migratoires\",u\"énormes\",u\"moyens\",u\"structures\",u\"loger\",u\"répondre\",u\"peuples\",u\"coopération\",u\"indéniable\",u\"réfugiés\",u\"digne\",u\"prendre la main\",u\"équilibrée\",u\"accueil\",u\"correctement\",u\"favorable\",u\"hébergés\",u\"mise à l’abri\",u\"centre humanitaire\",u\"situation\",u\"convenable\",u\"heurtée\",u\"valeurs\",u\"république\",u\"comportement\",u\"politique\",u\"associations\",u\"hébergement\",u\"respect\",u\"lien\",u\"confiance\",u\"travailleur\",u\"social\",u\"policiers\",u\"gazent\",u\"détruisent\",u\"nourriture\",u\"refusent\",u\"points d’eau\",u\"négation\",u\"partent\",u\"parcours\",u\"migration\",u\"terribles\",u\"touche\",u\"cimetière\",u\"choquant\",u\"scandaleux\",u\"dignité\",u\"impression\",u\"responsabilités\",u\"multiplication\",u\"conflits\",u\"excessivement\",u\"violents\",u\"solution\",u\"seule\",u\"quitter\",u\"victimes\",u\"esclavage\",u\"regression\",u\"humanité\",u\"guerre\",u\"état\",u\"pauvreté\",u\"pays\",u\"conditions\",u\"respect\",u\"organisme\",u\"ficher\",u\"contrôler\",u\"demande\",u\"papiers\",u\"droits\",u\"échec\",u\"aide\",u\"développement\",u\"vivre\",u\"famille\",u\"terrain\",u\"voir\",u\"souffrance\",u\"regression\",u\"France\",u\"richesse\",u\"humanité\",u\"extraordinaire\",u\"intérger\",u\"asile\",u\"question\",u\"temporaire\",u\"débat\",u\"souhaitons\",u\"propositions\",u\"européen\",u\"rapprocher\",u\"proches\",u\"position\",u\"absurdité\",u\"ONG\",u\"aider\",u\"détresse\",u\"lois\",u\"règles\",u\"principes\",u\"humanité\",u\"vulnérable\",u\"enfants\",u\"sauver\",u\"vies\",u\"générosité\",u\"classes\",u\"francophones\",u\"découverte\",u\"langue\",u\"logements\",u\"associations\",u\"continuerai\",u\"nouveaux\",u\"clé\",u\"chômage\",u\"héberger\",u\"employer\",u\"former\",u\"éduquer\",u\"milliers\",u\"vaincre\",u\"réticences\",u\"acccepter\",u\"aménage\",u\"refuges\",u\"régions\",u\"viennent\",u\"zones\",u\"protégées\",u\"normales\",u\"chez-eux\",u\"circuler\",u\"librement\",u\"Allemagne\",u\"obligations\",u\"humanitaire\",u\"préserver\",u\"populations\",u\"bataille\",u\"hégémonique\",u\"bataille\",u\"s'installer\",u\"mois\",u\"désaccord\",u\"moins\",u\"accueillant\",u\"bataille\",u\"idéologique\",u\"pression\",u\"chercher\",u\"immigrés\",u\"de force\",u\"délogés\",u\"expulsés\",u\"xénophobie\",u\"racistes\",u\"recul\",u\"idées\",u\"argent\",u\"dramatique\",u\"problèmes\",u\"économiques\",u\"interventions\",u\"militaires\",u\"famine\",u\"massacres\",u\"répression\",u\"bombardé\",u\"manqué\",u\"devoir\",u\"indigne\",u\"déposer\",u\"demande d'asile\",u\"absurde\",u\"campements\",u\"regrette\",u\"dispositifs\",u\"pérennes\",u\"divergence\",u\"appréciation\",u\"reconstitution\",u\"campements\",u\"crise\",u\"emballement\",u\"otage\",u\"noyer\",u\"guerre\",u\"bombardement\",u\"morts\",u\"difficultés\",u\"blocage\",u\"maltraiter\",u\"seul\",u\"privation\",u\"absurde\",u\"illusion\",u\"mensonge\",u\"ghettos\",u\"français\",u\"divisés\",u\"meurent\",u\"fuient\",u\"devoirs\",u\"moraux\",u\"centres de rétention\",u\"protection\",u\"cause\",u\"guerre\",u\"lieu\",u\"couteux\",u\"charge\",u\"services\",u\"gratuits\",u\"système\",u\"gestion\",u\"patrimoine\",u\"nationale\",u\"importante\",u\"fossé\",u\"travail\",u\"difficulté\",u\"maîtrise\",u\"suspendre\",u\"shengen\",u\"arrêter\",u\"immigration\",u\"communautarisme\",u\"fondamentalisme\",u\"islamiste\",u\"gel\",u\"constructions\",u\"mosquées\",u\"islam\",u\"radicaux\",u\"vivier\",u\"radicalisation\",u\"soldats\",u\"djihadisme\",u\"terroristes\",u\"puissance\",u\"terreau\",u\"frappent\",u\"clandestine\",u\"massive\",u\"insécurité\",u\"terrifiante\",u\"forces de l'ordre\",u\"mesures législatives\",u\"suppression\",u\"enrayer\",u\"processus\",u\"récupérer\",u\"clandestins\",u\"absence\",u\"bateaux\",u\"en mer\",u\"ramener\",u\"facilite\",u\"réseaux\",u\"criminels\",u\"laisser\",u\"mourir\",u\"sauver\",u\"ramener\",u\"port d’origine\",u\"corrompus\",u\"passeurs\",u\"arme\",u\"catastrophique\",u\"occupation\",u\"illégale\",u\"inexpulsable\",u\"engrenage\",u\"procédures\",u\"demandeurs\",u\"déboutés\",u\"économique\",u\"raisons\",u\"traverser\",u\"sortis\",u\"territoire\",u\"arrivés\",u\"origine\",u\"critères\",u\"extrêmement\",u\"incitatif\",u\"centres\",u\"AME\",u\"CMU\",u\"ensemble\",u\"folle\",u\"coût\",u\"faramineux\",u\"malvenu\",u\"opposition\",u\"opposée\",u\"communautarisme\",u\"fondamentalisme\",u\"imposer\",u\"ennemi\",u\"contre\",u\"rejet\",u\"anti-démocratique\",u\"enfermemement\",u\"séparation\",u\"pourissement\",u\"étranger\",u\"drame\",u\"faillite\",u\"déloyale\",u\"ménage\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigrationMots=sorted(set(immigration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compterVoyelles(chaine):\n",
    "    result=0\n",
    "    for element in chaine:\n",
    "        if element in voyelles:\n",
    "            result+=1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lister les groupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listerGroupes(repRacine):\n",
    "    return glob.glob(repRacine+ur\"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chercher les inconnus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicterInconnus(lInconnus):\n",
    "    result={}\n",
    "    for line in lInconnus:\n",
    "        line=line.strip()\n",
    "        p=line.split(\";\")\n",
    "        if len(p[1])!=0:\n",
    "            if len(p)>8:\n",
    "                print p\n",
    "                for i in range(len(p)+1,7):\n",
    "                    p.append(\"\")\n",
    "            result[p[0].lower()]=(p[0],p[1],p[2],p[3],p[4],p[5],p[6],p[7],p[8])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lireInconnus(repGroupe):\n",
    "    result={}\n",
    "    fInconnus=repGroupe+\"/inconnus.txt\"\n",
    "    try:\n",
    "        with codecs.open(fInconnus,\"r\",encoding=\"utf8\") as inFile:\n",
    "            lInconnus=inFile.readlines()\n",
    "        result=dicterInconnus(lInconnus)\n",
    "        print result\n",
    "    except:\n",
    "        print \"pas d'inconnus.txt\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lister les EAFs du groupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listerEAFs(repGroupe):\n",
    "    result=[f for f in glob.glob(repGroupe+ur\"/*.eaf\") if \"phonetique\" not in unidecode.unidecode(f.lower())]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lister les mots d'un EAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimer(mot):\n",
    "#     mot=mot.lower()\n",
    "    for p in u',;.-?!“”‘’‛‟′″´˝\"«»':      # Modifié le 12/01/20 pour gérer les deux points comme marqueur dans les mots\n",
    "        mot=mot.replace(p, ' ')\n",
    "    mot=mot.strip()\n",
    "    return mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deparentheser(mot):\n",
    "    forme=mot\n",
    "    graphie=mot\n",
    "    m=re.search(ur\"\\([^)]*\\)\",forme)\n",
    "    while (m):\n",
    "        forme=re.sub(ur\"\\(([^)]*)\\)\",\"\\g<1>\", forme)\n",
    "        graphie=re.sub(ur\"\\(([\\wâàéèêëîïôùûüçÂÀÉÈÊËÎÏÔÙÛÜÇæœÆŒ]+['’]?)\\)\",\"'\", graphie)\n",
    "        m=re.search(ur\"\\([^)]*\\)\",forme)\n",
    "    forme=forme.lstrip(\")\").rstrip(\"(\")\n",
    "    forme=forme.replace(\":\",\"\")\n",
    "    if graphie.endswith(\"(\"):\n",
    "        graphie=graphie[:-1]+\"'\"\n",
    "    return(forme,graphie)\n",
    "#    motsAbreges[mot]={\"lexical\":forme, \"graphie\":graphie}    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Nor(m)al'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimer(\"Nor(m)al\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizerTour(tour,lexique):\n",
    "    listeMots=[]\n",
    "    listeTokens=[]\n",
    "    elements=re.findall(ur\"[\\wâàéèêëîïôùûüçÂÀÉÈÊËÎÏÔÙÛÜÇæœÆŒ():]+|[-.…,—–()\\[\\]\\/#\\\"“”‘«»<>'’=~]| ?[;!?:]\", tour)\n",
    "    elements=[x for x in elements if x!=u\" \"]\n",
    "    mots=[x for x in elements if not x in u\"-_.…,—–()\\[\\]\\/#\\\"“”‘«»<>'’=~:\" and not x in [u\" ;\",u\" !\",u\" ?\",u\" :\"]]\n",
    "    for element in elements:\n",
    "        if element in mots:\n",
    "            mot = trimer(element)\n",
    "            (forme,graphie)=deparentheser(mot)\n",
    "            listeMots.append(graphie)\n",
    "            if forme.lower() in lexique:\n",
    "                listeTokens.append(lexique[forme.lower()])\n",
    "            else:\n",
    "                listeTokens.append(forme.lower())\n",
    "        else:\n",
    "            listeMots.append(element)\n",
    "            listeTokens.append(element)\n",
    "    return listeMots,listeTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizerTexte(texte):\n",
    "    listeMots=set()\n",
    "    elements=re.findall(ur\"[\\wâàéèêëîïôùûüçÂÀÉÈÊËÎÏÔÙÛÜÇæœÆŒ():]+|[-.…,—–()\\[\\]\\/#\\\"“”‘«»<>'’=~]| ?[;!?:]\", texte)\n",
    "    elements=[x for x in elements if x!=u\" \"]\n",
    "    mots=[x for x in elements if not x in u\"-_.…,—–()\\[\\]\\/#\\\"“”‘«»<>'’=~:\" and not x in [u\" ;\",u\" !\",u\" ?\",u\" :\"]]\n",
    "    for mot in mots:\n",
    "        mot = trimer(mot)\n",
    "        (forme,graphie)=deparentheser(mot)\n",
    "        listeMots.add(forme.lower())\n",
    "    return list(listeMots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listerMots(nomEAF):\n",
    "    xmlEAF=ET.parse(nomEAF,parser)\n",
    "    texteLignesEAF=[]\n",
    "    for tier in xmlEAF.xpath(\"//TIER\"):\n",
    "        for annotation in tier.xpath(\"ANNOTATION/ALIGNABLE_ANNOTATION\"):\n",
    "            aValue=annotation.xpath(\"ANNOTATION_VALUE/text()\")\n",
    "            texteLignesEAF.append(\"\\n\".join(aValue))\n",
    "    texteEAF=\"\\n\".join(texteLignesEAF)\n",
    "    return tokenizerTexte(texteEAF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faireLexique(nomEAF,groupeInconnus):\n",
    "    extraitBdlexique={}\n",
    "    listeMots=listerMots(nomEAF)\n",
    "    for forme in listeMots:\n",
    "        if forme in groupeInconnus:\n",
    "            extraitBdlexique[forme]=groupeInconnus[forme]\n",
    "        elif forme in bdlexique:\n",
    "            extraitBdlexique[forme]=bdlexique[forme]\n",
    "        else:\n",
    "            groupeInconnus[forme]=(forme,\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\")\n",
    "            extraitBdlexique[forme]=(forme,\"***%s***\"%forme,\"\",\"\",\"\",\"\",\"\",\"\",\"\")\n",
    "    return extraitBdlexique,groupeInconnus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicterContent(content,lexique):\n",
    "    lMots,lTokens=tokenizerTour(content,lexique)\n",
    "    result=[[mot,lTokens[nMot]] for nMot,mot in enumerate(lMots)]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changement de convention d'annotation pour les disfluences\n",
    "- 2021: \n",
    "    - [blablabla] => \\<disfluence\\>blablabla\\</disfluence\\>\n",
    "- 2022: \n",
    "    - [rire] => \\<rire/\\>\n",
    "    - {blablabla} => \\<disfluence\\>blablabla\\</disfluence\\>\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def baliserTour(tour,lexique):\n",
    "    lMorceaux=re.split(ur\"(\\[[^]]*\\])\",tour)\n",
    "    newTour=[]\n",
    "    for lMorceau in lMorceaux:\n",
    "        m=re.match(ur\"\\[(.*)\\]\",lMorceau)\n",
    "        if m:\n",
    "            bContent=m.group(1)\n",
    "            if bContent.startswith(u\"=\"):\n",
    "                newTour.append([\"[%s]\"%bContent,\"<%s>\"%bContent[1:]])\n",
    "            elif bContent.startswith(u\"/\"):\n",
    "                newTour.append([\"[%s]\"%bContent,\"</%s>\"%bContent[1:]])\n",
    "            else:\n",
    "                newTour.append([\"[\",\"<disfluence>\"])\n",
    "                bContentDict=dicterContent(bContent,lexique)\n",
    "                newTour.extend(bContentDict)\n",
    "                newTour.append([\"]\",\"</disfluence>\"])\n",
    "        else:\n",
    "            if lMorceau.strip()!=\"\":\n",
    "                lMorceauDict=dicterContent(lMorceau.strip(),lexique)\n",
    "                newTour.extend(lMorceauDict)\n",
    "    return newTour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeAttribs(chaine):\n",
    "    result=chaine\n",
    "    mAttribs=re.match(ur\"(\\w+) (((\\w+)=[\\\"\\']?[^\\\"\\']+[\\\"\\']?\\s*)+)\",chaine)\n",
    "    if mAttribs:\n",
    "        result=mAttribs.group(1)+\" \"+mAttribs.group(2)\n",
    "    else:\n",
    "        mType=re.match(ur\"(\\w+) (.*)\",chaine)\n",
    "        if mType:\n",
    "            result=mType.group(1)+\" type='%s'\"%mType.group(2)\n",
    "    return result \n",
    "\n",
    "def makeTagAttribs(chaine):\n",
    "    resTag=unidecode.unidecode(chaine)\n",
    "    resAttrib={}\n",
    "    mAttribs=re.match(ur\"(\\w+) (((\\w+)=[\\\"\\']?[^\\\"\\']+[\\\"\\']?\\s*)+)\",chaine,re.U)\n",
    "    if mAttribs:\n",
    "#         print mAttribs.groups()\n",
    "        resTag=unidecode.unidecode(mAttribs.group(1))\n",
    "        if \"'\" not in mAttribs.group(2) and '\"' not in mAttribs.group(2):\n",
    "            resAttrib=re.findall(ur\"(\\w+)=(\\S+)\",mAttribs.group(2),re.U)\n",
    "#             print resAttrib\n",
    "            resAttrib={unidecode.unidecode(k):v for k,v in resAttrib}            \n",
    "        else:\n",
    "            resAttrib=re.findall(ur\"(\\w+)=[\\\"\\']([^\\\"\\']+)[\\\"\\']\",mAttribs.group(2),re.U)\n",
    "#             print resAttrib\n",
    "            resAttrib={unidecode.unidecode(k):v for k,v in resAttrib}\n",
    "    else:\n",
    "        mType=re.match(ur\"(\\w+) (.*)\",chaine,re.U)\n",
    "        if mType:\n",
    "            resTag=unidecode.unidecode(mType.group(1))\n",
    "            resAttrib={\"type\":mType.group(2)}\n",
    "    return resTag,resAttrib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'e t=acc p=fR\\xe3sE'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "makeAttribs(u\"e t=acc p=fRãsE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baliserTour(tour,lexique):    \n",
    "    lMorceaux=re.split(ur\"(\\[[^]]*\\])\",tour)\n",
    "#     print lMorceaux\n",
    "    newTour=[]\n",
    "    for lMorceau in lMorceaux:\n",
    "        m1=re.match(ur\"\\[(.*)\\]\",lMorceau)\n",
    "        m2=re.match(ur\"\\{(.*)\\}\",lMorceau)\n",
    "        if m1:\n",
    "            bContent=m1.group(1)\n",
    "            if bContent.startswith(u\"=\"):\n",
    "                newTour.append([\"[%s]\"%bContent,\"<%s>\"%makeAttribs(bContent[1:])])\n",
    "            elif bContent.startswith(u\"/\"):\n",
    "                newTour.append([\"[%s]\"%bContent,\"</%s>\"%bContent[1:]])\n",
    "            else:\n",
    "                newTour.append([\"[%s]\"%bContent,\"<%s/>\"%makeAttribs(bContent)])\n",
    "        elif m2:\n",
    "            aContent=m2.group(1)\n",
    "            newTour.append([\"{\",\"<disfluence>\"])\n",
    "            aContentDict=dicterContent(aContent,lexique)\n",
    "            newTour.extend(aContentDict)\n",
    "            newTour.append([\"}\",\"</disfluence>\"])\n",
    "\n",
    "        else:\n",
    "            if lMorceau.strip()!=\"\":\n",
    "                lMorceauDict=dicterContent(lMorceau.strip(),lexique)\n",
    "                newTour.extend(lMorceauDict)\n",
    "    return newTour"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "testBaliseTour=baliserTour(u\"qu’est ce c’est ça [=CH] cette affaire Dupont là [/CH]\",subBdlexique)\n",
    "print testBaliseTour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trouverMot2(nMot1,tourBalises):\n",
    "    result=None\n",
    "    if len(tourBalises[nMot1])>1:\n",
    "#         print tourBalises[nMot1][1]\n",
    "        if isinstance(tourBalises[nMot1][1],tuple):\n",
    "            for nMot2,mot2 in enumerate(tourBalises[nMot1+1:]):\n",
    "                if len(mot2)>1:\n",
    "#                     print mot2[0],mot2[1]\n",
    "                    if isinstance(mot2[1],tuple):\n",
    "                        result=nMot1+1+nMot2\n",
    "                        break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faireTourPhon(tourBalises):\n",
    "    result=[]\n",
    "    for nMot1 in range(len(tourBalises)):\n",
    "        nMot2=trouverMot2(nMot1,tourBalises)\n",
    "        mot1=tourBalises[nMot1]\n",
    "#         print mot1\n",
    "        if nMot2:\n",
    "            mot2=tourBalises[nMot2]\n",
    "            contexteLiaison=liaison_obligatoire(mot1,mot2)\n",
    "#             print nMot1,mot1[1],contexteLiaison\n",
    "            result.append(formerLiaison(mot1,mot2,contexteLiaison))\n",
    "        else:\n",
    "#             print mot1\n",
    "            if len(mot1)==2 and isinstance(mot1[1],tuple):\n",
    "                result.append(sampa2api(mot1[1][1]))\n",
    "            elif len(mot1)==2:\n",
    "#                 print \"len(mot1)==2\", mot1\n",
    "                result.append(mot1[1])\n",
    "            else:\n",
    "#                 print \"len(mot1)!=2\", mot1\n",
    "                result.append(mot1[0])\n",
    "    return \"\".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcrireTourEAF(tour,lexique):\n",
    "    tourBalises=baliserTour(tour,lexique)\n",
    "    tourEAF=faireTourPhon(tourBalises)\n",
    "    return tourEAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcrireEAF(nomEAF,subBdlexique):\n",
    "    xmlEAF=ET.parse(nomEAF,parser)\n",
    "    texteLignesEAF=[]\n",
    "    for nAnnotation,annotation in enumerate(xmlEAF.xpath(\"//ANNOTATION_VALUE\")):\n",
    "        if annotation.text:\n",
    "            tourEAF=transcrireTourEAF(annotation.text,subBdlexique)\n",
    "            annotation.text=tourEAF\n",
    "    return xmlEAF\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enchasseMot(balise,mot,graphie,phono,nMot,nTour):\n",
    "    ortho=mot[0]\n",
    "    cat=mot[3]\n",
    "    if cat in [u\"J\",u\"K\"]:\n",
    "        cat=u\"Adj\"\n",
    "    ms=mot[4]\n",
    "    vs=mot[5]\n",
    "    lexeme=mot[6].upper()\n",
    "    freq=mot[8]\n",
    "    nbVoyelles=compterVoyelles(mot[1])\n",
    "    if u\" \" in vs:\n",
    "        vs=u\"\"\n",
    "    motAttributs={\"cat\":cat,\"ms\":ms,\"vs\":vs,\"phon\":phono,\"nbsyll\":str(nbVoyelles),\"ortho\":ortho, \"lexeme\":lexeme, \"freq\":freq, \"id\":\"%05d%03d\"%(nTour,nMot)}\n",
    "    baliseMotBDL=ET.SubElement(balise,\"motBDL\",motAttributs)\n",
    "    baliseMotBDL.text=graphie\n",
    "    return nbVoyelles\n",
    "    \n",
    "\n",
    "def enchasseNonMot(balise,nonMot):\n",
    "    lNonMot=ET.SubElement(balise, \"punct\")\n",
    "    lNonMot.text=nonMot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mapBalises={\"???\":\"incomprehensible\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enchasserTourTRS(tour,tourMots):\n",
    "    nbVoyelles=0\n",
    "    nbMots=0\n",
    "    lConnecteurs=set()\n",
    "    lThemeMots=set()\n",
    "#     print tourMots\n",
    "#     print tour\n",
    "\n",
    "    nTour=int(tour.attrib[\"id\"])\n",
    "    stackBalises=[tour]\n",
    "    for nMot1,mot1 in enumerate(tourMots):\n",
    "#         print nMot1,mot1\n",
    "        graphie=mot1[0]\n",
    "        lexMot=mot1[1]\n",
    "        if lexMot:\n",
    "            orthoMot=lexMot[0]\n",
    "        else:\n",
    "            orthoMot=\"\"\n",
    "        if orthoMot in connecteurs:\n",
    "            lConnecteurs.add(orthoMot)\n",
    "        if orthoMot in lThemeMots:\n",
    "            lThemeMots.add(orthoMot)\n",
    "        if isinstance(lexMot,tuple):\n",
    "            nbMots+=1\n",
    "            nMot2=trouverMot2(nMot1,tourMots)\n",
    "            phono=\"???\"\n",
    "            if nMot2:\n",
    "                mot2=tourMots[nMot2]\n",
    "                contexteLiaison=liaison_obligatoire(mot1,mot2)\n",
    "#                 print nMot1,mot1[1],contexteLiaison\n",
    "                phono=formerLiaison(mot1,mot2,contexteLiaison)\n",
    "                \n",
    "            nbVoyelles+=enchasseMot(stackBalises[-1],lexMot,graphie,phono,nMot1,nTour)\n",
    "        else:\n",
    "            m=re.match(ur\"</.*>\",lexMot)\n",
    "            if m:\n",
    "                stackBalises.pop(-1)\n",
    "#                 print \"pop\",groupeEAF\n",
    "#                 print tourMots\n",
    "#                 print stackBalises[-1].tag\n",
    "            else:\n",
    "                m=re.match(ur\"<(.*[^/])/?>\",lexMot)\n",
    "                if m:\n",
    "                    locBalise=m.group(1)\n",
    "#                     print \"stackBalises\",ET.tostring(stackBalises[-1]),locBalise\n",
    "                    if locBalise in mapBalises:\n",
    "                        locBalise=mapBalises[locBalise]\n",
    "                    locTag,locAttribs=makeTagAttribs(locBalise)\n",
    "                    newBalise=ET.SubElement(stackBalises[-1],locTag,locAttribs)\n",
    "                    stackBalises.append(newBalise)\n",
    "#                     print \"push\",stackBalises[-1].tag\n",
    "                else:\n",
    "                    enchasseNonMot(stackBalises[-1],lexMot)\n",
    "    tour.attrib[\"nbSyll\"]=str(nbVoyelles)\n",
    "    tour.attrib[\"nbMots\"]=str(nbMots)\n",
    "    if lConnecteurs:\n",
    "        tour.attrib[\"connecteurs\"]=\" \".join(lConnecteurs)\n",
    "    if lThemeMots:\n",
    "        tour.attrib[\"themeMots\"]=\" \".join(lThemeMots)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Turn>\n",
      "  <tour id=\"00001\">\n",
      "    <mot>mot1</mot>\n",
      "    <nom>\n",
      "      <mot>nom1</mot>\n",
      "    </nom>\n",
      "    <mot>mot2</mot>\n",
      "  </tour>\n",
      "</Turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "turn=ET.Element(\"Turn\")\n",
    "tour=ET.SubElement(turn,\"tour\",id=\"00001\")\n",
    "e1=ET.SubElement(tour,\"mot\")\n",
    "e1.text=\"mot1\"\n",
    "nom1=ET.SubElement(tour,\"nom\")\n",
    "nom1e1=ET.SubElement(nom1,\"mot\")\n",
    "nom1e1.text=\"nom1\"\n",
    "e2=ET.SubElement(tour,\"mot\")\n",
    "e2.text=\"mot2\"\n",
    "print ET.tostring(turn,pretty_print=True,encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcrireTRS(nomEAF,lexique):\n",
    "    xmlEAF=ET.parse(nomEAF,parser)\n",
    "\n",
    "    #################\n",
    "    #\n",
    "    # Récupération des timestamps\n",
    "    #\n",
    "    #################\n",
    "    \n",
    "    ts={}\n",
    "    for timeOrder in xmlEAF.xpath(\"//TIME_ORDER/TIME_SLOT\"):\n",
    "        tsID=timeOrder.attrib[\"TIME_SLOT_ID\"]\n",
    "        tsTime=timeOrder.attrib[\"TIME_VALUE\"]\n",
    "        ts[tsID]=tsTime\n",
    "    tiersTypes={}\n",
    "    annotations={}\n",
    "    turnTextes=[]\n",
    "\n",
    "    #################\n",
    "    #\n",
    "    # Récupération des tours\n",
    "    #\n",
    "    #################\n",
    "    \n",
    "    for tier in xmlEAF.xpath(\"//TIER\"):\n",
    "        tierID=tier.attrib[\"TIER_ID\"]\n",
    "        tierType=tier.attrib[\"LINGUISTIC_TYPE_REF\"]\n",
    "        tiersTypes[tierID]=tierType\n",
    "        for annotation in tier.xpath(\"ANNOTATION/ALIGNABLE_ANNOTATION\"):\n",
    "            aID=annotation.attrib[\"ANNOTATION_ID\"]\n",
    "            aTS1=annotation.attrib[\"TIME_SLOT_REF1\"]\n",
    "            aTS2=annotation.attrib[\"TIME_SLOT_REF2\"]\n",
    "            aBegin=time2TRS(ts[aTS1])\n",
    "            aEnd=time2TRS(ts[aTS2])\n",
    "            aValue=annotation.xpath(\"ANNOTATION_VALUE/text()\")\n",
    "            turnTextes.append(\"\\n\".join(aValue))\n",
    "            turn=(tierID, aBegin, aEnd, aValue)\n",
    "            annotations[aID]=turn\n",
    "            \n",
    "\n",
    "    #################\n",
    "    #\n",
    "    # Fabrication entête TRS\n",
    "    #\n",
    "    #################\n",
    "\n",
    "    root=ET.Element(\"Trans\")\n",
    "    speakers=ET.SubElement(root, \"Speakers\")\n",
    "    speakerID={}\n",
    "    for nSpk,spk in enumerate(tiersTypes):\n",
    "        ET.SubElement(speakers,\"Speaker\",id=\"spk%d\"%(nSpk+1),name=spk)\n",
    "        speakerID[spk]=\"spk%d\"%(nSpk+1)\n",
    "\n",
    "    episode=ET.SubElement(root, \"Episode\")\n",
    "    section=ET.SubElement(episode, \"Section\")   \n",
    "    tourId=0\n",
    "    for k,v in sorted(annotations.iteritems(),key=lambda x: int(x[0].strip(\"a\"))):\n",
    "        #\n",
    "        # créer la structure Turn avec speaker, startTime, endTime\n",
    "        # et tour avec id\n",
    "        #\n",
    "        turn=ET.SubElement(section,\"Turn\",speaker=speakerID[v[0]],startTime=v[1],\\\n",
    "                                          endTime=v[2])\n",
    "        tour=ET.SubElement(turn,\"tour\",id=\"%05d\"%tourId)\n",
    "        #\n",
    "        # découper le texte de l'annotation\n",
    "        #\n",
    "        vTextes=baliserTour(\"\\n\".join(v[3]),lexique)\n",
    "        if debug: print vTextes\n",
    "        enchasserTourTRS(tour,vTextes)\n",
    "        tourId+=1\n",
    "    tree = ET.ElementTree(root)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecrireInconnus(dInconnus,repGroupe):\n",
    "    fInconnus=repGroupe+\"/inconnus.txt\"\n",
    "    with codecs.open(fInconnus,\"w\",encoding=\"utf8\") as outFile:\n",
    "        for inconnu in dInconnus.values():\n",
    "            outFile.write(\";\".join(inconnu)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/BernardCantin', u'/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/ChavierChristophePigeot', u'/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/CoiffardRioWilburn', u'/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/GARYMARREL', u'/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/Wang', u'/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/AntonLaujac ', u'/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/DosSantosLabbe', u'/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/BegayLapeyreLapeyre', u'/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/GonzalezTostain', u'/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/AndrieuxAntoinetteBernede', u'/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/DupouyFavreMartins', u'/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/GatienHardouinPonthier', u'/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/AuriaultBartolomucciBrettes', u'/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/BarsacqHazeraRegazzoni', u'/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/BennadjemaHamon', u'/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/EtaleGilliot', u'/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/BonnefonAlbrespy', u'/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/LamourouxPrevotRichardson', u'/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/ALBANELCHAMPERNAU', u'/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/BernouChamardBravo', u'/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/SoattoZappino']\n"
     ]
    }
   ],
   "source": [
    "repRacine=\"/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-20%d/0-Groupes/\"%annee\n",
    "repGroupes=listerGroupes(repRacine)\n",
    "print repGroupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/BernardCantin\n",
      "{}\n",
      "groupeEAF BernardCantintranscriptionortho2.eaf\n",
      "groupeEAF BernardCantintranscriptionortho3.eaf\n",
      "groupeEAF BernardCantintranscriptionortho4.eaf\n",
      "groupeEAF BernardCantintranscriptionortho5.eaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gilles/opt/anaconda3/envs/python2/lib/python2.7/site-packages/unidecode/__init__.py:46: RuntimeWarning: Argument <type 'str'> is not an unicode object. Passing an encoded string will likely have unexpected results.\n",
      "  _warn_if_not_unicode(string)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groupeEAF BernardCantintranscriptionortho6.eaf\n",
      "groupeEAF invitation-à-boire-2.eaf\n",
      "groupeEAF invitation à boire 1.eaf\n",
      "groupeEAF anniversaire.eaf\n",
      "groupeEAF repasentreamis.eaf\n",
      "groupeEAF wklyon.eaf\n",
      "groupeEAF BernardCantintranscriptionortho7.eaf\n",
      "groupeEAF Clapi_Signal_repas___conversation_repas_francais_vrai_4f4b911396.eaf\n",
      "groupeEAF transcription 1 - invitation à jouer (jeux vidéo).eaf\n",
      "groupeEAF BernardCantin transcriptions orthographiques.eaf\n",
      "groupeEAF conversation-tel-québec.eaf\n",
      "groupeEAF BernardCantintranscriptionsortho1.eaf\n",
      "\n",
      "/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/ChavierChristophePigeot\n",
      "{}\n",
      "groupeEAF partie charlotte elan .eaf\n",
      "groupeEAF Enregistrement partie Jade .eaf\n",
      "groupeEAF Enregistrement.eaf\n",
      "groupeEAF Enregistrement partie Ines.eaf\n",
      "\n",
      "/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/CoiffardRioWilburn\n",
      "{}\n",
      "groupeEAF Roman.eaf\n",
      "groupeEAF homophobiechezlesjeunes_phonetic.eaf\n",
      "groupeEAF cacommenceaujourdhui_fin.eaf\n",
      "groupeEAF homophobiechezlesjeunes.eaf\n",
      "groupeEAF 2020.eaf\n",
      "groupeEAF 1979.eaf\n",
      "groupeEAF 1973.eaf\n",
      "groupeEAF 1984.eaf\n",
      "groupeEAF cacommenceaujourdhui_debut.eaf\n",
      "\n",
      "/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/GARYMARREL\n",
      "{}\n",
      "groupeEAF S1Y2_Laurine.eaf\n",
      "groupeEAF S2Y2_Laurine.eaf\n",
      "groupeEAF S3Y2_Laurine.eaf\n",
      "groupeEAF S5Y2_Laurine.eaf\n",
      "groupeEAF S6Y2_Laurine.eaf\n",
      "groupeEAF S7Y2_Laurine.eaf\n",
      "groupeEAF S8Y2_Laurine.eaf\n",
      "groupeEAF S10Y2_Laurine.eaf\n",
      "groupeEAF S11Y2_Laurine.eaf\n",
      "groupeEAF S12Y2_Laurine.eaf\n",
      "groupeEAF S1Y3_Estelle.eaf\n",
      "groupeEAF S2Y3_Estelle.eaf\n",
      "groupeEAF S3Y3_Estelle.eaf\n",
      "groupeEAF S4Y3_Estelle.eaf\n",
      "groupeEAF S5Y3_Estelle.eaf\n",
      "groupeEAF S6Y3_Estelle.eaf\n",
      "groupeEAF S7Y3_Estelle.eaf\n",
      "groupeEAF S8Y3_Estelle.eaf\n",
      "groupeEAF S9Y3_Estelle.eaf\n",
      "groupeEAF S10Y3_Estelle.eaf\n",
      "groupeEAF S11Y3_Estelle.eaf\n",
      "groupeEAF S12Y3_Estelle.eaf\n",
      "groupeEAF S4Y2_Laurine.eaf\n",
      "groupeEAF S9Y2_Laurine.eaf\n",
      "\n",
      "/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/Wang\n",
      "[u'j\\xe8te', u'ZEt', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'puisqu', u'pHisk@', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'interlaken', u'\\xeatERlak\\xe2', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'anim', u'anim', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'navigationnelle', u'navikasjiOnEl', u'', u'', u'', u'', u'', u'', u'']\n",
      "pas d'inconnus.txt\n",
      "groupeEAF WANG Jiaying.eaf\n",
      "groupeEAF ZHU-Zili.eaf\n",
      "groupeEAF CHEN Xinxin.eaf\n",
      "\n",
      "/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/AntonLaujac \n",
      "{}\n",
      "groupeEAF Histoire_Grenouille_5ans_1_.eaf\n",
      "groupeEAF Histoire_Grenouille_5ans_2_.eaf\n",
      "groupeEAF Histoire_Grenouille_5ans_3_.eaf\n",
      "groupeEAF Histoire_Grenouille_5ans_4.eaf\n",
      "groupeEAF Histoire_Grenouille_6ans_2.eaf\n",
      "groupeEAF Histoire_Grenouille_6ans_3.eaf\n",
      "groupeEAF Histoire_Grenouille_6ans_4.eaf\n",
      "groupeEAF Histoire_Grenouille_6ans_5.eaf\n",
      "groupeEAF Histoire_Grenouille_6ans_6.eaf\n",
      "groupeEAF Histoire_Grenouille_7ans_1.eaf\n",
      "groupeEAF Histoire_Grenouille_7ans_2.eaf\n",
      "groupeEAF Histoire_Grenouille_7ans_3 .eaf\n",
      "groupeEAF Histoire_Grenouille_7ans_4.eaf\n",
      "groupeEAF Histoire_Grenouille_10ans_2_.eaf\n",
      "groupeEAF Histoire_Grenouille_10ans_1_.eaf\n",
      "groupeEAF Histoire_Grenouille_10ans_3_.eaf\n",
      "\n",
      "/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/DosSantosLabbe\n",
      "[u'bilefelt', u'bilfild', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'kramer', u'k\\u0281am\\u0153\\u0281', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'mouais', u'mwe', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'psycho', u'psiko', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'niquel', u'nik\\u025bl', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'rf1', u'\\u0281f\\xfb', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'3', u't\\u0281wa', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'humm', u'\\u0259m', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'bayonne', u'baj\\u0254n\\u0259', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'madona', u'madona', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'9', u'n\\u0153f', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'12', u'duz\\u0259', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'tandis', u't\\xe2di', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'jeanne', u'\\u0292an\\u0259', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'oulala', u'ulala', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'lala', u'lala', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'gut', u'g\\u0153t', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'afrique', u'af\\u0281ik', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'ass', u'ass', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'mmmmm', u'm', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'larzac', u'la\\u0281zak', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'80', u'kat\\u0281\\u0259v\\xea', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'yes', u'j\\u025bs', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'heineken', u'\\u025bjn\\xf8k\\u025bn', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'afghanistan', u'afganist\\xe2', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'asie', u'azi', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'urss', u'u\\u0281ss', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'pfff', u'pf', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'woh', u'wo', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'new', u'nju', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'europe', u'\\xf8\\u0281\\u0254p\\u0259', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'irak', u'i\\u0281ak', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'martin', u'ma\\u0281t\\xea', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'nanani', u'nanani', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'nanana', u'nanana', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'marzat', u'ma\\u0281za', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'oula', u'ula', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'zediaque', u'zedjak', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'montepellier', u'm\\xf4p\\xf8lje', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'rda', u'\\u0281da', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'z\\xe9lande', u'zel\\xe2d\\u0259', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'bosh', u'b\\u0254\\u0283', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'pauillac', u'pojak', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'syrie', u'si\\u0281i', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'york', u'j\\u0254\\u0281k', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'co', u'ko', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'montpellier', u'm\\xf4p\\xf8lje', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'trentre', u't\\u0281\\xe2t\\u0259', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'ss20', u'ssv\\xea', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'fance', u'f\\u0281\\xe2s', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'bielefeld', u'bilfild', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'tannenbaum', u'tan\\u025bnb\\xfbm', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'city', u'siti', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'france', u'f\\u0281\\xe2s', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'2', u'd\\xf8', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'ouh', u'u', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'6', u'sis', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'4', u'kat\\u0281\\u0259', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'dalas', u'dalas', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'pff', u'pf', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'hoffman', u'\\u0254fman', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'11', u'\\xf4z\\u0259', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'10', u'dis', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'beh', u'be', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'october', u'\\u0254kt\\u0254b\\u0281', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'aix', u'\\u025bks', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'allemagne', u'alma\\u0272', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'am\\xe9rique', u'ame\\u0281ik', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'russie', u'\\u0281ysi', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'lepen', u'l\\xf8p\\u025bn', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'mickey', u'mike', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'srda', u's\\u0281da', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'pourcent', u'pu\\u0281s\\xe2', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'35', u't\\u0281\\xe2t\\u0259s\\xeak', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'zeland', u'zel\\xe2d', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'mouse', u'maws', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'40', u'ka\\u0281\\xe2t\\u0259', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'5', u's\\xeak', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'audi', u'odi', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'nice', u'nis', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'dustin', u'd\\u0153stin', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'5\\xe8me', u's\\xeakj\\u025bm', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'olala', u'\\u0254lala', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'provence', u'p\\u0281ov\\xe2s', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'peugot', u'p\\xf8\\u0292o', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'gallil\\xe9', u'galile', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'francais', u'f\\u0281\\xe2se', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'fac', u'fak', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'rennaud', u'\\u0281\\xf8no', u'', u'', u'', u'', u'', u'', u'']\n",
      "[u'michel', u'mi\\u0283\\u025bl', u'', u'', u'', u'', u'', u'', u'']\n",
      "{u'bilefelt': (u'bilefelt', u'bilfild', u'', u'', u'', u'', u'', u'', u''), u'kramer': (u'kramer', u'k\\u0281am\\u0153\\u0281', u'', u'', u'', u'', u'', u'', u''), u'fac': (u'fac', u'fak', u'', u'', u'', u'', u'', u'', u''), u'co': (u'co', u'ko', u'', u'', u'', u'', u'', u'', u''), u'psycho': (u'psycho', u'psiko', u'', u'', u'', u'', u'', u'', u''), u'niquel': (u'niquel', u'nik\\u025bl', u'', u'', u'', u'', u'', u'', u''), u'rf1': (u'rf1', u'\\u0281f\\xfb', u'', u'', u'', u'', u'', u'', u''), u'4': (u'4', u'kat\\u0281\\u0259', u'', u'', u'', u'', u'', u'', u''), u'humm': (u'humm', u'\\u0259m', u'', u'', u'', u'', u'', u'', u''), u'jeanne': (u'jeanne', u'\\u0292an\\u0259', u'', u'', u'', u'', u'', u'', u''), u'bayonne': (u'bayonne', u'baj\\u0254n\\u0259', u'', u'', u'', u'', u'', u'', u''), u'12': (u'12', u'duz\\u0259', u'', u'', u'', u'', u'', u'', u''), u'tandis': (u'tandis', u't\\xe2di', u'', u'', u'', u'', u'', u'', u''), u'dustin': (u'dustin', u'd\\u0153stin', u'', u'', u'', u'', u'', u'', u''), u'oulala': (u'oulala', u'ulala', u'', u'', u'', u'', u'', u'', u''), u'lala': (u'lala', u'lala', u'', u'', u'', u'', u'', u'', u''), u'martin': (u'martin', u'ma\\u0281t\\xea', u'', u'', u'', u'', u'', u'', u''), u'z\\xe9lande': (u'z\\xe9lande', u'zel\\xe2d\\u0259', u'', u'', u'', u'', u'', u'', u''), u'ass': (u'ass', u'ass', u'', u'', u'', u'', u'', u'', u''), u'ouh': (u'ouh', u'u', u'', u'', u'', u'', u'', u'', u''), u'mmmmm': (u'mmmmm', u'm', u'', u'', u'', u'', u'', u'', u''), u'larzac': (u'larzac', u'la\\u0281zak', u'', u'', u'', u'', u'', u'', u''), u'80': (u'80', u'kat\\u0281\\u0259v\\xea', u'', u'', u'', u'', u'', u'', u''), u'yes': (u'yes', u'j\\u025bs', u'', u'', u'', u'', u'', u'', u''), u'afghanistan': (u'afghanistan', u'afganist\\xe2', u'', u'', u'', u'', u'', u'', u''), u'asie': (u'asie', u'azi', u'', u'', u'', u'', u'', u'', u''), u'urss': (u'urss', u'u\\u0281ss', u'', u'', u'', u'', u'', u'', u''), u'pfff': (u'pfff', u'pf', u'', u'', u'', u'', u'', u'', u''), u'3': (u'3', u't\\u0281wa', u'', u'', u'', u'', u'', u'', u''), u'woh': (u'woh', u'wo', u'', u'', u'', u'', u'', u'', u''), u'new': (u'new', u'nju', u'', u'', u'', u'', u'', u'', u''), u'europe': (u'europe', u'\\xf8\\u0281\\u0254p\\u0259', u'', u'', u'', u'', u'', u'', u''), u'irak': (u'irak', u'i\\u0281ak', u'', u'', u'', u'', u'', u'', u''), u'gut': (u'gut', u'g\\u0153t', u'', u'', u'', u'', u'', u'', u''), u'nanani': (u'nanani', u'nanani', u'', u'', u'', u'', u'', u'', u''), u'bosh': (u'bosh', u'b\\u0254\\u0283', u'', u'', u'', u'', u'', u'', u''), u'york': (u'york', u'j\\u0254\\u0281k', u'', u'', u'', u'', u'', u'', u''), u'nanana': (u'nanana', u'nanana', u'', u'', u'', u'', u'', u'', u''), u'marzat': (u'marzat', u'ma\\u0281za', u'', u'', u'', u'', u'', u'', u''), u'oula': (u'oula', u'ula', u'', u'', u'', u'', u'', u'', u''), u'zediaque': (u'zediaque', u'zedjak', u'', u'', u'', u'', u'', u'', u''), u'35': (u'35', u't\\u0281\\xe2t\\u0259s\\xeak', u'', u'', u'', u'', u'', u'', u''), u'rda': (u'rda', u'\\u0281da', u'', u'', u'', u'', u'', u'', u''), u'afrique': (u'afrique', u'af\\u0281ik', u'', u'', u'', u'', u'', u'', u''), u'5\\xe8me': (u'5\\xe8me', u's\\xeakj\\u025bm', u'', u'', u'', u'', u'', u'', u''), u'pauillac': (u'pauillac', u'pojak', u'', u'', u'', u'', u'', u'', u''), u'syrie': (u'syrie', u'si\\u0281i', u'', u'', u'', u'', u'', u'', u''), u'mouais': (u'mouais', u'mwe', u'', u'', u'', u'', u'', u'', u''), u'montpellier': (u'montpellier', u'm\\xf4p\\xf8lje', u'', u'', u'', u'', u'', u'', u''), u'trentre': (u'trentre', u't\\u0281\\xe2t\\u0259', u'', u'', u'', u'', u'', u'', u''), u'ss20': (u'ss20', u'ssv\\xea', u'', u'', u'', u'', u'', u'', u''), u'fance': (u'fance', u'f\\u0281\\xe2s', u'', u'', u'', u'', u'', u'', u''), u'bielefeld': (u'bielefeld', u'bilfild', u'', u'', u'', u'', u'', u'', u''), u'tannenbaum': (u'tannenbaum', u'tan\\u025bnb\\xfbm', u'', u'', u'', u'', u'', u'', u''), u'city': (u'city', u'siti', u'', u'', u'', u'', u'', u'', u''), u'france': (u'france', u'f\\u0281\\xe2s', u'', u'', u'', u'', u'', u'', u''), u'2': (u'2', u'd\\xf8', u'', u'', u'', u'', u'', u'', u''), u'dalas': (u'dalas', u'dalas', u'', u'', u'', u'', u'', u'', u''), u'6': (u'6', u'sis', u'', u'', u'', u'', u'', u'', u''), u'pff': (u'pff', u'pf', u'', u'', u'', u'', u'', u'', u''), u'hoffman': (u'hoffman', u'\\u0254fman', u'', u'', u'', u'', u'', u'', u''), u'11': (u'11', u'\\xf4z\\u0259', u'', u'', u'', u'', u'', u'', u''), u'10': (u'10', u'dis', u'', u'', u'', u'', u'', u'', u''), u'beh': (u'beh', u'be', u'', u'', u'', u'', u'', u'', u''), u'october': (u'october', u'\\u0254kt\\u0254b\\u0281', u'', u'', u'', u'', u'', u'', u''), u'aix': (u'aix', u'\\u025bks', u'', u'', u'', u'', u'', u'', u''), u'heineken': (u'heineken', u'\\u025bjn\\xf8k\\u025bn', u'', u'', u'', u'', u'', u'', u''), u'am\\xe9rique': (u'am\\xe9rique', u'ame\\u0281ik', u'', u'', u'', u'', u'', u'', u''), u'russie': (u'russie', u'\\u0281ysi', u'', u'', u'', u'', u'', u'', u''), u'lepen': (u'lepen', u'l\\xf8p\\u025bn', u'', u'', u'', u'', u'', u'', u''), u'mickey': (u'mickey', u'mike', u'', u'', u'', u'', u'', u'', u''), u'srda': (u'srda', u's\\u0281da', u'', u'', u'', u'', u'', u'', u''), u'pourcent': (u'pourcent', u'pu\\u0281s\\xe2', u'', u'', u'', u'', u'', u'', u''), u'montepellier': (u'montepellier', u'm\\xf4p\\xf8lje', u'', u'', u'', u'', u'', u'', u''), u'zeland': (u'zeland', u'zel\\xe2d', u'', u'', u'', u'', u'', u'', u''), u'madona': (u'madona', u'madona', u'', u'', u'', u'', u'', u'', u''), u'audi': (u'audi', u'odi', u'', u'', u'', u'', u'', u'', u''), u'mouse': (u'mouse', u'maws', u'', u'', u'', u'', u'', u'', u''), u'40': (u'40', u'ka\\u0281\\xe2t\\u0259', u'', u'', u'', u'', u'', u'', u''), u'5': (u'5', u's\\xeak', u'', u'', u'', u'', u'', u'', u''), u'9': (u'9', u'n\\u0153f', u'', u'', u'', u'', u'', u'', u''), u'nice': (u'nice', u'nis', u'', u'', u'', u'', u'', u'', u''), u'olala': (u'olala', u'\\u0254lala', u'', u'', u'', u'', u'', u'', u''), u'provence': (u'provence', u'p\\u0281ov\\xe2s', u'', u'', u'', u'', u'', u'', u''), u'peugot': (u'peugot', u'p\\xf8\\u0292o', u'', u'', u'', u'', u'', u'', u''), u'allemagne': (u'allemagne', u'alma\\u0272', u'', u'', u'', u'', u'', u'', u''), u'francais': (u'francais', u'f\\u0281\\xe2se', u'', u'', u'', u'', u'', u'', u''), u'rennaud': (u'rennaud', u'\\u0281\\xf8no', u'', u'', u'', u'', u'', u'', u''), u'michel': (u'michel', u'mi\\u0283\\u025bl', u'', u'', u'', u'', u'', u'', u''), u'gallil\\xe9': (u'gallil\\xe9', u'galile', u'', u'', u'', u'', u'', u'', u'')}\n",
      "groupeEAF audio2_dossantos_labbe.eaf\n",
      "groupeEAF audio3_dossantos_labbe.eaf\n",
      "groupeEAF audio1_dossantos_labbe.eaf\n",
      "\n",
      "/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/BegayLapeyreLapeyre\n",
      "{}\n",
      "groupeEAF Audio7-Repas_avec_papy_Guytou.eaf\n",
      "groupeEAF Audio2-Repas_Arzacq_Partie1.eaf\n",
      "groupeEAF Audio3-Repas_Arzacq_Partie2.eaf\n",
      "groupeEAF Audio4-Hypothese_aeroport.eaf\n",
      "groupeEAF Audio1-Papy_et_Mamie_telephone.eaf\n",
      "groupeEAF Audio6-Papy_Guytou_telephone.eaf\n",
      "groupeEAF Audio5-Monologue Papy Pardoux.eaf\n",
      "\n",
      "/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/GonzalezTostain\n",
      "{}\n",
      "groupeEAF TostainGonzalez_audio.eaf\n",
      "groupeEAF GonzalezTostain.eaf\n",
      "groupeEAF GonzalezTostain1 .eaf\n",
      "groupeEAF GonzalezTostain2 .eaf\n",
      "groupeEAF GonzalezTostain3 .eaf\n",
      "groupeEAF GonzalezTostain4 .eaf\n",
      "groupeEAF GonzalezTostain5 .eaf\n",
      "groupeEAF GonzalezTostain6 .eaf\n",
      "\n",
      "/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/AndrieuxAntoinetteBernede\n",
      "{}\n",
      "groupeEAF HOT ONES  Jamel Debbouze avale une bombe lacrymo.eaf\n",
      "groupeEAF Vanessa Paradis interview.eaf\n",
      "groupeEAF CORPUS.eaf\n",
      "groupeEAF Catherine Deneuve.eaf\n",
      "groupeEAF Vanessa Paradis.eaf\n",
      "groupeEAF CORPUS 2.eaf\n",
      "groupeEAF CORPUS 3.eaf\n",
      "groupeEAF CORPUS1.eaf\n",
      "\n",
      "/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/DupouyFavreMartins\n",
      "{}\n",
      "groupeEAF DupouyFavreMartins_Christophe-1.eaf\n",
      "groupeEAF DupouyFavreMartins_Christophe-2.eaf\n",
      "groupeEAF DupouyFavreMartins_francais-africain.eaf\n",
      "groupeEAF DupouyFavreMartins_francais-canadien.eaf\n",
      "groupeEAF DupouyFavreMartins_Christophe-4.eaf\n",
      "\n",
      "/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/GatienHardouinPonthier\n",
      "{}\n",
      "groupeEAF audio_1.eaf\n",
      "groupeEAF podcast_3.eaf\n",
      "groupeEAF audio_2.eaf\n",
      "groupeEAF audio_4.eaf\n",
      "groupeEAF podcast_1.eaf\n",
      "groupeEAF podcast_2.eaf\n",
      "groupeEAF podcast_4.eaf\n",
      "groupeEAF audio_3.eaf\n",
      "\n",
      "/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/AuriaultBartolomucciBrettes\n",
      "{}\n",
      "groupeEAF football Orthographique.eaf\n",
      "groupeEAF Triathlon Orthographique.eaf\n",
      "groupeEAF AuriaultBartolomucciBrettes_transcription.eaf\n",
      "\n",
      "/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/BarsacqHazeraRegazzoni\n",
      "{}\n",
      "groupeEAF Ambre.eaf\n",
      "groupeEAF Anaelle.eaf\n",
      "groupeEAF Antoine.eaf\n",
      "groupeEAF Busra.eaf\n",
      "groupeEAF Coline.eaf\n",
      "groupeEAF Maxime.eaf\n",
      "groupeEAF Orianne.eaf\n",
      "groupeEAF Ambre-art.eaf\n",
      "groupeEAF Melissa 1.eaf\n",
      "groupeEAF Basil.eaf\n",
      "groupeEAF Bastien.eaf\n",
      "groupeEAF Elise.eaf\n",
      "groupeEAF Eva.eaf\n",
      "groupeEAF gabriel.eaf\n",
      "groupeEAF Hannah.eaf\n",
      "groupeEAF Jade.eaf\n",
      "groupeEAF Julia .eaf\n",
      "groupeEAF Lauriane.eaf\n",
      "groupeEAF Laurine.eaf\n",
      "groupeEAF Marc.eaf\n",
      "groupeEAF Marie.eaf\n",
      "groupeEAF Matteo.eaf\n",
      "groupeEAF Sarah.eaf\n",
      "groupeEAF Louis.eaf\n",
      "\n",
      "/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/BennadjemaHamon\n",
      "{}\n",
      "groupeEAF audio-4-adultes.eaf\n",
      "groupeEAF audio-1-enfants.eaf\n",
      "groupeEAF audio-2-enfants.eaf\n",
      "groupeEAF audio-3-enfants.eaf\n",
      "groupeEAF audio-6-insultes.eaf\n",
      "\n",
      "/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/EtaleGilliot\n",
      "{}\n",
      "groupeEAF premier enregistrement.eaf\n",
      "\n",
      "/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/BonnefonAlbrespy\n",
      "{}\n",
      "groupeEAF Transcription_orthographique1.eaf\n",
      "groupeEAF Transciption_orthographique2.eaf\n",
      "\n",
      "/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/LamourouxPrevotRichardson\n",
      "{}\n",
      "groupeEAF 18 ans - H - LEA_Chloe.eaf\n",
      "groupeEAF Camille - 16 ans - lycee - F_Chloe.eaf\n",
      "groupeEAF H - 20 ans - pas d'emploi_Chloe.eaf\n",
      "groupeEAF L1 hist art 18 M.eaf\n",
      "groupeEAF Master MEEF - 21 - F_Chloe.eaf\n",
      "groupeEAF Yanis - 21 ans - H - alternance webdesign_Chloe.eaf\n",
      "groupeEAF 2emeanneeMEEFFemme22.eaf\n",
      "groupeEAF Enzoacteur20anshomme.eaf\n",
      "groupeEAF yanis17lyceecrado.eaf\n",
      "groupeEAF 2emeAnneeMEEF-22-F.eaf\n",
      "groupeEAF Celeste-physique-20-F.eaf\n",
      "groupeEAF Melissa-DoctoranteAnthropologie-25-35-F.eaf\n",
      "groupeEAF yanis17lycéecrado.eaf\n",
      "groupeEAF Enzo-ecoledacteur-20-homme.eaf\n",
      "groupeEAF MEEF-21-F.eaf\n",
      "groupeEAF Yanis-21-H-webdesign.eaf\n",
      "groupeEAF 18-H-LEA.eaf\n",
      "groupeEAF Camille-16-lycee-F.eaf\n",
      "groupeEAF L1-HistArt-18-M.eaf\n",
      "groupeEAF Alexandre23radioH.eaf\n",
      "groupeEAF L2lettresmodernes20 ans.eaf\n",
      "groupeEAF PiaFmusique22.eaf\n",
      "groupeEAF Tristan-lycee-16-H.eaf\n",
      "groupeEAF H-pasdemploi-20.eaf\n",
      "groupeEAF L1-culture-humaniste-scientifique-chs-19-M.eaf\n",
      "groupeEAF Pia-musique-22-F.eaf\n",
      "groupeEAF Alexandre-23-H-radio.eaf\n",
      "groupeEAF L2-lettresmodernes-20-F.eaf\n",
      "groupeEAF H-17-lycée.eaf\n",
      "groupeEAF meefFavuTheodanstram.eaf\n",
      "groupeEAF M2-MEEF-LettresModernes-22-F.eaf\n",
      "groupeEAF Lucas-17-2nd-H.eaf\n",
      "groupeEAF Lucas - 17 ans - lycee 2nd - H (1)_TheoChloe.eaf\n",
      "groupeEAF L1-chs-19-F.eaf\n",
      "groupeEAF Caroline-geographie-20-F.eaf\n",
      "\n",
      "/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/ALBANELCHAMPERNAU\n",
      "{}\n",
      "groupeEAF Rush Ep2_Leandre.eaf\n",
      "groupeEAF Temoin.eaf\n",
      "groupeEAF Rush Ep2_Fanny.eaf\n",
      "\n",
      "/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/BernouChamardBravo\n",
      "{}\n",
      "groupeEAF Yuanzhe_Zili_Tr_ortho.eaf\n",
      "groupeEAF Enregistrement Mascha.eaf\n",
      "groupeEAF Sabrina_Roberta_Pamela_Tr_ORTHO.eaf\n",
      "\n",
      "/Users/gilles/pCloud Drive/FOD/GB/Cours/L2-Corpus/Corpus-2022/0-Groupes/SoattoZappino\n",
      "{}\n",
      "groupeEAF transcription-course-motogp-australie.eaf\n",
      "groupeEAF transcription-course-motogp-valence.eaf\n"
     ]
    }
   ],
   "source": [
    "debug=0\n",
    "for repGroupe in repGroupes[:]:\n",
    "    print\n",
    "    print repGroupe\n",
    "    groupeInconnus=lireInconnus(repGroupe)\n",
    "    groupeEAFs=listerEAFs(repGroupe)\n",
    "    for groupeEAF in groupeEAFs:\n",
    "        print \"groupeEAF\",groupeEAF.split(\"/\")[-1]\n",
    "        subBdlexique,groupeInconnus=faireLexique(groupeEAF,groupeInconnus)\n",
    "        newEAF=transcrireEAF(groupeEAF,subBdlexique)\n",
    "        fNewEAF=groupeEAF.replace(\".eaf\",\"-phonetique.eaf\")\n",
    "        newEAF.write(fNewEAF, pretty_print=True, encoding='utf-8', xml_declaration=True)\n",
    "        newTRS=transcrireTRS(groupeEAF,subBdlexique)\n",
    "        nomXML=groupeEAF.replace(\".eaf\",\".xml\")\n",
    "        if nomXML!=groupeEAF:\n",
    "            newTRS.write(nomXML, pretty_print=True, encoding='utf-8', xml_declaration=True)\n",
    "        else:\n",
    "            print \"pb de nom EAF\",nomEAF\n",
    "    ecrireInconnus(groupeInconnus,repGroupe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
